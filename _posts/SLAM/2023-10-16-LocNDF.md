---
title : "[논문 리뷰] LocNDF: Neural Distance Field Mapping for Robot Localization"
category :
    - SLAM
tag :
    - SLAM
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---
[2023, IEEE RA-L] LocNDF: Neural Distance Field Mapping for Robot Localization 논문 리뷰

# Abstract
**Mapping**은 여러 robotic task에 필수인 환경이다. 특히 **localization**에 필수적이다.

본 논문에서는 robot localization에 매우 적합한 map 표현(representation)을 얻는 것을 목표로 LiDAR point cloud를 사용하여 환경을 mapping하는 문제를 다룬다.

이를 위해, **neural network를 활용**하여 localization을 위해 주어진 장면의 discretization free **distance field**를 학습한다.

LiDAR 센서 데이터를 직접 사용하는 방법을 제안하고 있으며, 환경에 대한 기존 모델이나 추정에 의존하지 않는다.

**NeRF representation**에서 영감을 받아 measured beam을 따라 샘플링된 점에 의해 네트워크를 다루며,<br>
valid **distance field**를 학습하도록 설계되었다.

또한, neural distance field(NDF)에서 직접 global localization과 scan registration을 수행하는 것을 보인다.

<br>

# Introduction
reconstruction과 같은 작업은 가능한 정확하게 scene을 복제하려고 노력한다.

제안한 map representation에서 로봇의 localization 성능을 개선하기 위해 센서 데이터로부터의 point clouds를 이용해 환경을 representation하는 문제를 다룬다.

mobile robotics에서 보통의 map representation은 occupancy maps, surfels, distance fields, NDTs(Normal distributions transform) 그리고 raw point cloud 등이 있다.<br>
(이러한 표현들은 효율적 관리를 위해 KD-tree, grid map, hash map등의 자료구조를 적절히 사용한다.)

distance field를 통한 scene representation은 distance라는 것이 scan registration, MCL, path planning 혹은 reconstruction에 있어 이점을 가진다.

**Nerual distance field(NDF)**는 simple **MLP**로 모델링되었고, 공간 내 지점과 가장 가까운 표면 사이의 거리를 나타낸다.

**network의 연속성**으로 인해 일반적인 grid 기반의 distance field에 비해 장점을 가지고, grid resolution에 의해 제한되지 않는다.

또한, **networks는 저차원의 정보를 효율적으로 표현**할 수 있는 능력이 있는 반면, grid 기반 representation은 벽이나 ground plane같은 저차원 객체를 저장하는데 많은 셀을 사용한다.

보통 computer vision분야에서는 NDF의 학습은 고해상도의 GT meshes 혹은 normal(법선) 정보를 사용하거나 주어진 directly superveised by a given distance file이다.

지금까지 raw LiDAR 데이터와 같은 raw sensor data에서 NDF를 어떻게 훈련될 수 있는지는 거의 조사되지 않았다.

또한 이러한 NDF가 mobile robotic task에 얼마나 적합한지도 명확하지 않다.

---

본 논문에서 **real LiDAR sensor**로부터 얻은 point cloud를 기반으로 NDF를 학습하는 것을 보인다.

또한, NDF에서 localization하는 것을 보이고, 자율 주행 영역에서 주어진 맵에서의 odometry 추정을 위해 poise tracking을 살핀다.

실내 환경의 경우, MCL을 사용한 **global localization**을 살핀다.

본 논문의 세 가지 contribution은 다음과 같다.

- 본 논문에서 제안한 loss function은 NDF를 학습할 수 있게 한다.
- 이는 scan registration과 
- MCL을 통한 localization을 가능하게 하는데 매우 적합하다.

<p align="center"><img src="/MyPDF/locndf(2).png" width = "600" ></p>

<br>

# Learning Neural Distance Fields For Robot Localization
LiDAR나 RGB-D를 통해 얻은 point cloud를 통해 명시적으로 localization을 돕기 위한 NDF를 학습한다.

point cloud normal에 의존하지 않는다. normal estimation은 센서의 종류엨 크게 의존하고 오류가 발생하기 쉽기 때문이다.

언급한 거리에 관련한 센서들은 센서 원점으로부터 표면까지의 거리를 측정하는 공통점이 있다.

본 논문의 유일한 가정은 그 사이의 공간이 free space라는 점이다.

NDF 학습 방법을 설명하고, 두 가지 common localization 기법을 보인다.<br>
(scan registration using ICP and global localization using a particle filter)

## Learning NDF From Sensor Data
목표는 closest surface에 대해 distance $d$를 얻기 위해 space $p \in \mathbb{R}^{\mathcal{D}}$내의 임의의 점에서 NDF $D$를 query하는 것이다.

outdoor robotics를 위해 차원은 $\mathcal{D} = 3$이다. 이는 일반적으로 사용되는 automative LiDAR sensor 또는 terrestrial laser scanner를 사용하여 localziation과 registration을 다룬다.

뿐만 아니라, $\mathcal{D} = 2$를 사용하는 indoor environment에서도 localization 및 navigation을 위해 일반적으로 장착된 2D LiDAR를 사용한다.

본 논문의 map의 representation은 multi-layer perceptron(이하 **MLP**) $D : \mathbb{R}^{\mathcal{D}} \mapsto \mathbb{R}$이다.<br>
이는 **input coordiantes를 Euclidean distance space로 mapping**한다.

**NeRF**에서 영감받은 것에 따라 [**Postional Encoding**](../../MyPDF/locndf(1).png)을 적용한다. $ \pi : \mathbb{R}^{\mathcal{D}} \mapsto \mathbb{R}^{2I}$<br>
([NeRF 논문리뷰_Positional Encoding](https://lee-jaewon.github.io/deep_learning_study/NeRF/#positional-encoding))

<p align="center"><img src="/MyPDF/locndf(1).png" width = "600" ></p>

Postional Encoding 적용 이유는 보통 NeRF에서 밝히길 **입력 좌표를 직접 사용하는 것이 high-frequency 변동을 잘 표현하지 못하고** 결과가 좋지 않아 사용했다고 밝힌다.

그래서 [$\pi(p)$](../../MyPDF/locndf(1).png)와 같은 periodic encoding 함수를 사용하여 **더 높은 차원으로의 매핑**($\mathbb{R}^{\mathcal{D}} \mapsto \mathbb{R}^{2I}$)을 수행한다.<br>
(aim to retain high-frequency information in the distance field)<br>
(reference도 직접적으로 NeRF 논문이 달려있다.)

ground truth distance, occupancy field, given normals에 훈련 과정을 supervise한 이전의 접근 방식과는 대조적으로,

Truncated signed distance field(TSDF) fusion 파이파라인과 유사한 LiDAR 센서의 측정 프로세스를 활용한다.

TSDF 방식을 직접 사용하지는 않고 다음과 같은 방법으로 approximated distance를 설명한다.

Laser sensor는 센서 원점 부터 표면까지의 거리를 측정한다.(이를 ray distance $d_p$로 나타낸다.)

또한, NeRF에서 영감을 받아 LiDAR beam을 따라 다음과 같이 점들을 sampling한다.

$$ [ p_i \in \mathbb{R}^{D} \mid i = 1,...,N_i ] $$

$$ p_i=(1-\lambda_{i})o_i + \lambda_{i}e_i $$

sensor origin : $o_i \in \mathbb{R}^{\mathcal{D}}$, end of the beam : $e_i \in \mathbb{R}^{\mathcal{D}}$

**surface의 근접한 곳(근처)에서** 더 많은 points를 sampling하기 위해 아래와 같은 log-linearly sampling을 사용한다.<br>
(NeRF도 투과율이 낮은 곳(물체가 있는 곳)에서 더 많은 점들을 sampling한다.)

<!-- $\lambda_{i} = {1-10^{i \over N-1} \over 0.9}$ -->

$$\lambda_{i} = {1 - 10^{(i / (N-1)) - 1} \over 0.9}$$

결국, 표면에 대한 각 sampled 포인트의 ray distance $d_i$는 $d_i = \lVert e_i - p_i \rVert$이다.

$d_i$가 closest surface까지의 distance와 꼭 일치할 필요는 없다.

surface에서 가장 가까운 sampling된 점을 찾는 것은 대규모 지도에서는 계산 비용이 많이 들고, reconstruction을 통해 표면을 결정해야 하는데, 이는 복잡할 수 있다.

표면에서 가장 가까운 점을 찾는 대신 gradient를 이용한 NDF인 $D$를 사용하여 가장 가까운 표면까지의 방향 $n_i$를 분석적으로 근사화할 수 있다.

$$n_i = -{\partial D(p_i) \over \partial p_i}$$

이는 [Fig. 2.](../../MyPDF/locndf(3).png)에 나타나있다.

<p align="center"><img src="/MyPDF/locndf(3).png" width = "600" ></p>

**Gradient**는 우리에게 distance field의 가장 가파른 증가방향을 제공하며(point $p_i$에 대해), negative gradient는 가장 가까운 표면을 가리킨다.

실제로 $n_i$를 계산하기 위해 automatic differentiation을 사용할 수 있다.

거리 $d_i$를 방향 $n_i$를 따라 표면에 가장 가까운 **근사화된 distance**로 project할 수 있다.

$$ \tilde{d_i} = {(e_i-p_i)^{\top}n_i \over \lVert n_i \rVert} $$

학습에 이제 approximated distance $\tilde{d_i}$를 사용한다.

approximated distance $\tilde{d_i}$가 좋을수록 NDF를 더 잘 supervise할 수 있다.

동시에 NDF가 좋을수록 surface에 대한 방향을 더 잘 추정할 수 있으며, 최종적으로 더 나은 approximated distance가 될 것이다.<br>
(Circular problem)

이러한 circular dependency는 특히 NDF인 $D$를 무작위 가중치로 초기화할 때 학습이 안정적인지에 관한 의문을 제기할 수 있다.

실제로 교육중에 이 근사화로 인한 불안정성을 어떤 식으로도 관찰하지 못했다고 주장한다.

본 논문은 이것이 query지점 $p_i$가 surface에 더 가까울수록 근사 오차 

$$\epsilon = \left\vert \tilde{d_i}-d_i \right\vert $$

가 작기 때문이라고 한다.

또한, surface points or points가 surface에 가깝다면 $d_i \approx 0$이 되며, gradient $n_i$에 상관없이 $\epsilon_i \approx 0$에 도달한다.

그러므로, NDF를 학습하는 동안 올바른 거리가 surface에서 free space로 전파된다.

TSDF fusion 파이프라인과 비슷하게,<br>
작은 거리 $d_i$일때의 $\epsilon_i$의 우선순위를 정한다.

이를 위해 다음과 같이 가중치 $w_i$를 설정한다.

$$ w_i = (d_{max} - d_i)^{\gamma} $$

해석하면, 표면에 가장 가까운 $d_i$를 찾기 위한 가중치라는 것을 알 수 있다.

$d_{max}$는 해당 batch에서 가장 큰 거리값이다. $d_i$가 $d_{max}$에 가깝다면 weight는 작아진다.

$\gamma$는 hyper parameter인데, 클수록 far point에 대한 impact는 줄어든다.

NDF를 supervise하는 것은 weighted L1 loss를 통해 minimize한다.

[1. distance에 관한 loss](../../MyPDF/locndf(4).png)는 다음과 같다.

<p align="center"><img src="/MyPDF/locndf(4).png" width = "600" ></p>

$w_j$의 합은 포인트 클라우드의 점에 할당된 모든 가중치의 합계를 나타낸다.(정규화느낌)

[2. endpoint에 관한 loss](../../MyPDF/locndf(5).png)는 다음과 같다. end point가 표면에 놓여있는 것을 강제해야 하는 추가적인 loss이다.

<p align="center"><img src="/MyPDF/locndf(5).png" width = "600" ></p>

[ref : SHINE-Mapping](https://ieeexplore.ieee.org/abstract/document/10160907)과 비슷하게 [3. 정규화 손실(regularization loss)](../../MyPDF/locndf(6).png)을 추가하여 valid한 distance field임을 강제하는 Eikonal 방정식 $\left\vert  \nabla N \right\vert = 1$ 을 적용한다.

<p align="center"><img src="/MyPDF/locndf(6).png" width = "600" ></p>

(Eikonal Term은 네트워크 출력값에서 부호 있는 거리 값이 정확하게 표시되도록 하기 위한 위함수이다. 이는 이전에 모델링된 영역에 과도한 영향을 미치지 않도록 로컬 특징 벡터의 업데이트 방향을 제한함으로써 이루어진다. : 본 논문에서는 명시적으로 밝히진 않고 valid distance field만 enforce하기 위해 사용된다고 적혀있다.)

이미 gradient를 취했으니 $n_i$를 그냥 사용한 것 같고, 아래 첨자 2는 뭔지 나와있지 않다. 아마 Eiknol term이라는 것을 명시한게 아닌가 싶다.

또한, [4. 인접한 점들이 유사한 normal을 가진다는 것을 강제](../../MyPDF/locndf(7).png)하기 위한 손실도 있다.

<p align="center"><img src="/MyPDF/locndf(7).png" width = "600" ></p>

$\angle(\cdot)$은 cosine distance이다. 

$\hat{n_j}$는 이웃 $p_j$의 gradient이다.

이웃 $p_j$는 거리 $\tau$내에 있는 sampling된 점들이다. 다음 조건부에서 무작위로 선택한다.

$$ p \mid \lVert p_i-p < \tau  \rVert $$

최종 loss는 linear combination으로 이루었고, 다음과 같다.

<p align="center"><img src="/MyPDF/locndf(8).png" width = "600" ></p>

## Scan Registration Using a NDF
학습된 NDF를 사용하여 ICP를 효과적으로 사용하여 point cloud를 map에 registration하는 방법을 보여준다.

목표는 point cloud $P$를 NDF map인 $D$에 align하는 $R$과 $t$를 찾아내는 것이다.

이는 surface와 point cloud간의 거리를 줄이는 것이다.

<p align="center"><img src="/MyPDF/locndf(9).png" width = "600" ></p>

모든 점들이 어떤 R, t를 가질 때 NDF의 출력의 합들이 가장 작은 R*, t*를 구하는 것.<br>
([유클리디안 거리가 최소화 되도록 하는 R, t를 찾는 것이 point cloud registration](https://taeyoung96.github.io/slam/SLAM_03_1/#2-point-cloud-registration%EC%9D%B4%EB%9E%80))

이를 non-linear least squares optimization으로 풀었고, i번째 점에 대한 Jacobian은 다음과 같다.

<p align="center"><img src="/MyPDF/locndf(10).png" width = "600" ></p>

[식(11)](../../MyPDF/locndf(9).png)과 [식(12)]((../../MyPDF/locndf(10).png))에서 볼 수 있듯이, 해당 방법은 **corresponding point에 의존하지 않는다.**

이는 classical한 ICP-based method와 대조되며, 굳이 correspondence를 찾을 필요가 없는 것이 이점이다.

단지 문제를 solve하기 위해 direction($n$)과 얼마나 먼지(how far given by $D(p)$)만 알면 된다.

이 정보는 네트워크의 가중치들로 직접 encoding되고 맵의 생성과정에서 학습된다.

이에 대한 2D 시각화는 [다음](../../MyPDF/locndf(11).png)과 같다.

<p align="center"><img src="/MyPDF/locndf(11).png" width = "600" ></p>

---

Eiknoal equation $\left\vert  \nabla N \right\vert = 1$ 을 통해 distance field가 이를 만족해야한다는 것을 언급했다.<br>
(쉽게 말하면 지표면에서 1m 떨어진 곳으로 이동하면 거리는 1m 증가할 필요성이 있다 라고 소개한다.)

하지만 본 논문은 neural network에 의하여 NDF의 근사치만 추정하기 때문에 이것이 반드시 성립하지는 않는다.

만약 norm of the gradient가 1보다 더 크거나 작다면, 거리를 over or underestimate할 수 있다.

이러한 현상에 대응하기 위해, $D(p_i) / \lVert n_i \rVert$ norm으로 거리를 정규화하는 것을 제안한다.

만약 $\left\vert  \nabla N \right\vert > 1$ 이라면 거리를 overestimate하고 overshoot이 날 수도 있다.<br>
(이러면 surface에 도달하지 못한다고 한다.)

그래서 정규화를 하면 step이 shortening되어 보다 신뢰성 있는 registration을 가능하게 한다고 한다.

## MCL-Based Localization Using a NDF
이 파트에서는, MCL(Monte Carlo localization)을 이용하여 NDF내에서 **globally하게 localize**하는 방법을 소개한다.

time $t$에서 robot의 position $x_t$의 belief $bel(x_t)$는 particle의 집합으로 표현된다.

$w_t^i$ 에 대응하는

$$ (x_t^i, w_t^i) \mid i = 1,...,I $$

paritcle 집합.(MCL 내용이다.)

**motion model** $p(x_t) ~ p(x_t \mid x_{t-1}, u_t)$은 이전 위치 $x_{t-1}$ 과 control command $u_t$에 기반하여 particle들을 업데이트 한다.

particle들의 weight는 **observation model** $ w_t^i \propto p(z_t \mid x_t, D)  $에 의해 업데이트 된다.

**observation model** $ w_t^i \propto p(z_t \mid x_t, D)  $은 observations $z_t$, pose $x_t$, NDF $D$에 의존한다.

LiDAR 센서를 통해 local surrounding을 관찰할 수 있고, particle filter들 중 한 지점 기준의(i텀들) observed point cloud(j텀들)에서의 distance field를 구할 수 있다.

observed point cloud(j텀들) : $^j z_t^i | j = 1,...J$ <br>
-> i번째 particle에서 관찰한 point cloud set $z$ 

$$ ^j d_{t}^{i} = D(R_{t}^{i} \ \ {}^{j} z_t^i + t_t^i) $$

NDF를 통해 얻은 distance field는 표면을 결정한다. 실제 observation과 위 식에서 얻은 $^j d_{t}^{i}$ 사이의 평균거리가 낮을 수록 표면에 잘 맞아 있는 것이기에 가중치가 증가한다.

[Figure 4.](../../MyPDF/locndf(12).png)를 살펴보면 이해가 된다.

<p align="center"><img src="/MyPDF/locndf(12).png" width = "600" ></p>

weight 업데이트는 [아래 식]((../../MyPDF/locndf(13).png))을 통해 수행한다.

<p align="center"><img src="/MyPDF/locndf(13).png" width = "600" ></p>

$\beta$와 $\alpha$는 보통 outlier에 대해 강건하기 위해 사용하는 parameter라고만 소개한다.

weight 업데이트 식을 살펴보면, 거리 합의 평균(J개로 나눈 것)의 음수에 exponential을 취한다. 

그러면 평균이 커질 수록, exp(-x) 값은 작아지므로, 작은 가중치를 받게 된다.

이후, weight를 기준으로 resampling된다. (가능성이 가장 높은 위치(or particle)에 초점을 맞추기 위해 : MCL 내용임 _ Reseed)

이러한 이유들을 통해 global localization을 수행할 수 있다고 말하는 것 같다.<br>
(초기 위치 없이 뿌려진 particle들에서 직접 NDF를 통해 평가하기 때문?)

<br>

# EXPERIMENTAL EVALUATION
넘어가도록 하겠다.

<br>

# 참고 자료

- [Wiesmann, Louis, et al. "LocNDF: Neural Distance Field Mapping for Robot Localization." IEEE Robotics and Automation Letters (2023).](https://ieeexplore.ieee.org/document/10168941/)
- [Zhong, Xingguang, et al. "Shine-mapping: Large-scale 3d mapping using sparse hierarchical implicit neural representations." 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023](https://ieeexplore.ieee.org/abstract/document/10160907)
- [GitHub : LocNDF](https://github.com/PRBonn/LocNDF)
- [LeeJaewon's Blog : [논문 리뷰] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://lee-jaewon.github.io/deep_learning_study/NeRF/)
- [Taeyoung's Blog : Slam 3-1강 (Point Cloud Registration & ICP algorithm) 요약](https://taeyoung96.github.io/slam/SLAM_03_1/)

📣<br>
포스팅에서 오류나 궁금한 점은 Comments를 작성해 주시면, 많은 도움이 됩니다.💡
{: .notice--info}



