---
title : "강화학습 개요(2)"
category :
    - Reinforcement_Learning
tag :
    - Reinforcement_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---

강화학습을 구현하기 위해 기본적인 용어를 먼저 알아보자.(2)

[이전 포스팅](https://lee-jaewon.github.io/reinforcement_learning/RL_intro/)에 이어 강화학습 개념을 살펴보겠다.

> 강화 학습의 목적은 보상을 최대화하는 의사결정 방법을 학습하는 것이라고 했다.

### 가치함수
보상을 최대화할 수 있는 행동이란  
현재 State에서 이동할 수 있는(Grid World 예시) 다음 상태 중에서 가장 가치가 높은 상태로 이동할 수 있는 행동을 의미한다.

가치가 높다는 것은 그 state에 대한 반환값의 기대값이 높다는 것을 의미한다.

이 가치를 수식적으로 표현하기 위해 **가치 함수**라는 개념을 도입한다.

> 가치 함수 : 특정 상태에 대한 가치를 도출하는 함수. 상태를 입력으로 받고 그 상태의 가치를 출력한다.
일반적으로 가치 함수는 $V$로 표기한다.

즉, 다음 step의 보상이 모두 같더라도 그 다음 step의 보상이 더 클 가능성이 있는 곳으로, 즉 가치가 높은 곳으로 가기 위한 과정이다.

수식으로 표현하면 다음과 같다.  
$$ V(s) = E[R_{t+1}+\gamma R_{t+2}+{\gamma}^2 R_{t+3}+{\gamma}^3 R_{t+4} + \cdots|S_{t}=s] $$

현재 상태 $s$에서 Agent주변의 가치가 높은 상태로만 이동하면 목표에 도달할 수 있다.

최적의 행동을 가치 함수를 통해 표현하면 다음과 같다.  
$$ a^{*}=argmax_{a\in A}V(s') $$

수식의 의미는 현재 상태 $s$에서 취할 수 있는 행동들 A중에서  
최적의 행동 $a^{*}$는 가장 높은 가치를 얻을 수 있는 다음 상태 $s'$으로 이동할 수 있는 행동 a를 의미한다.

### Q function
가치 함수가 **상태에 대한 가치**를 도출했었다면,  
큐 함수는 **상태에서 특정 행동에 대한 가치**를 도출합니다.

따라서 큐 함수는 입력으로 상태와 행동을 받고 해당 상태에서 **행동에 대한 가치**를 출력한다.

일반적으로 큐 함수는 $Q$라고 표기한다.  
큐 함수를 수식으로 표현하면 다음과 같다.

$$ Q(s,a) = E[G_{t}|S_{t}=s, A_{t} = a] $$

즉, 현재 상태 $s$에서 행동 $a$를 통해 얻을 수 있는 반환 값들의 평균을 의미한다.

### 벨만 방정식
잘 학습된 가치 함수, 최적의 가치 함수가 있다면 **상태**마다 최적의 행동이 무엇인지 알 수 있고,

이를 통해 보상을 최대화 할 수 있다.  

그렇다면 가치 함수는 어떻게 학습할 수 있을까?

가치 함수는 벨만 방정식을 이용해 업데이트한다.  
이는 강화학습에서 상당히 주요한 부분을 차지하는 식이다.

벨만 방정식은 현재 상태의 가치함수와 다음 상태의 가치 함수 사이의 관계를 표현한 것이다.

$$ v_{\pi}(S)=E_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s] $$

위 식에서 첫 번째 항인 $R_{t+1}$은 시간 t에서 취한 행동에 대한 보상 값이며,  
두 번째 항인 $\gamma v_{\pi}(S_{t+1})$은 감가율을 적용한 가치 함수 값을 의미한다.

즉 현재 상태의 가치 함수를 다음 상태의 가치 함수로 표현한 다음, 기댓값으로 나타낸 것이다.

---

큐 함수 또한 벨만 기대 방정식으로 표현할 수 있다.

큐 함수에 대한 벨만 기대 방정식은 다음과 같다.

$$ q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a] $$

위 방정식으로 업데이트하다 보면 양변의 차이가 0으로 수렴하게 된다.

즉 현재 정책 $\pi$에 대한 참 가치 함수 혹은 큐 함수를 구할 수 있다.

하지만 강화학습의 목적은 최적의 의사결정을 하여 보상을 최대화하는 것이다.  
그렇기에 현재 정책에 대한 참 가치 함수가 아닌 최적 가치 함수를 찾아야 한다.

최적의 가치 함수는 모든 가능한 정책 중 제일 높은 가치를 반환하는 가치 함수를 의미한다.  
수식은 다음과 같다.

$$ v_{*}(s)=\underset{\pi}{max} v_{\pi}(s) $$

최적의 큐 함수는 다음과 같다.

$$ q_{*}(s,a)=\underset{\pi}{max} q_{\pi}(s,a) $$

---

최적의 가치 함수를 다음 상태의 가치 함수를 이용하여 식으로 나타내면 벨만 최적 방정식이 나타난다.

$$ v_{*}(s)=\underset{a}{max} E[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s,A_{t}=a] $$

최적의 큐 함수에 대해서도 벨만 최적 방정식으로 나타내면 다음과 같다.

$$ q_{*}(s,a)= E[R_{t+1}+\gamma \underset{a'}{max} q_{*}(S_{t+1},a')|S_{t}=s,A_{t}=a] $$

### 마무리
가치 함수와 큐 함수를 벨만 기대 방정식과 벨만 최적 방정식으로 표현하는 방법을 살펴봤는데,

솔직히 무슨 식인지 이 식이 기하학적으로 어떻게 작동하는지 와닿지 않는다.

개요 포스팅이기도 하고, 이론적인 내용이지만 다른 포스팅에서 조금 더 이 식들의 의미나 실제 코드로는 어떻게 돌아가는지 다뤄야할 것 같다.

📣<br> 
포스팅에서 오류나 궁금한 점은 Comments를 작성해주시면, 많은 도움이 됩니다.💡
{: .notice--info}