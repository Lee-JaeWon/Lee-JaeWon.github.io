---
title : "강화학습 개요"
category :
    - Reinforcement_Learning
tag :
    - Reinforcement_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---

강화학습을 구현하기 위해 기본적인 용어를 먼저 알아보자.

### Reinforcement Learning
기계 학습(Machine Learning)에 관한 자세한 내용은 담지 않겠다.

먼저, 강화 학습 또한 기계 학습의 유형중 하나이다.

우리가 Deep_Learning에서 공부한 것과는 달리,   

우리가 처음 자전거를 탈 때, 여러 번 넘어지고 실패하며 중심을 잡는 능력을 획득하는 것처럼  
강화 학습 또한 자전거를 타는 과정에서 여러 번의 시행착오를 거쳐 더 나은 방향을 위해 보상을 얻으며, 더 자전거를 잘 탈 수 있도록 하는 행동을 얻기 위한 것이라 생각하면 된다.

강화 학습이 우리가 생각하는 것보다 다양하고 많은 문제를 해결한다는 점에 있어 주목해볼만하다.

### 강화학습의 기본 용어
강화학습은 **보상**이라는 것을 최대화하는  
의사결정전략 -> 즉, 순차적인 행동을 알아나가는 방법이다.

여기서 순차적으로 계속 행동을 결정해야 하는 문제를 수학적으로 정의한 것이  
MDP(Markov Decision Process), 마크로프 결정 과정이다.

MDP의 구성 요소를 포함한 강화학습의 기본 용어를 살펴보자.

> - Agent : 강화학습에서 의사결정을 하는 역할. 예를 들어, 게임 내에서의 주인공이라 생각할 수 있다.
> - Environment : 에이전트의 의사결정을 반영하고, 에이전트에게 정보를 주는 역할을 한다.
예를 들어, 게임에서 Agent의 정보를 알려주는 화면(시스템)이라 할 수 있다.  
> - Observation : 관측은 환경에서 제공해주는 정보이다. 즉, 주인공이 어디에 있는지 이미지로 혹은 수치적으로 표현해줄 수 있다.
> - State : Agent는 상태를 기반으로 의사결정을 한다. 상태는 의사결정을 하기위해 관측값, 행동, 보상을 가공한 정보이다.  
일반적으로 현재 스텝의 상태는 $$ S_{t} $$라고 한다.
> - Action : 행동은 Agent가 의사결정을 동해 취할 수 있는 것을 의미한다. 일반적으로 현재 상태에서 취하는 행동을 $$ A_{t} $$라고 한다.
> - Reward Function : 보상함수는 에이전트가 특정 상태에서 특정 행동을 했을 때 보상을 받게 되고 에이전트는 이 보상 정보를 통해 학습을 진행한다.  
일반적으로 현재 상태 $s$에서 특정 행동 $a$를 했을 때 얻는 보상의 기댓값을 $$ R^{a}_{s} $$라고 한다. 

Reward Function을 수식으로 표현하면 다음과 같다.  
$$ R^{a}_{s} = E[R_{t+1} | S_{t}=s, A_{t}=a] $$  

$t$는 현재 스텝을 의미한다.

예를 들면, 로봇이 왼쪽으로 이동하면 보상 1을 얻고 아닌 경우 0을 얻는다고 했을 때,  
다음과 같은 함수의 형태로 표현할 수 있다.  
$$ R^{a}_{s} = \begin{cases}
1, & \mbox{if }a\mbox{ = left} \\
0, & \mbox{}\mbox{otherwise}
\end{cases} $$ 

### 감가율과 반환 값
그렇다면 시행착오를 겪으면서 보상을 최대화하는 의사결정 전략을 학습하는 것이 강화학습이라면,

어떻게 의사결정이라는 것을 학습할 수 있을까?

강화학습에서는 에피소드가 끝나면 지나왔던 상태에서 했던 행동에 대해 정보를 기록한다.  
그 정보를 이용하여 그 다음 에피소드에서 다시 의사결정을 한다.

이러한 에피소드를 반복하면서 계속 업데이트를 해준다.  
그렇다면 좋은 의사결정을 하기 위해서는 어떤 정보를 기록해줘야할까?

바로 **미래에 대한 정보**를 미리 알면 좋은 의사결정을 할 수 있을 것이다.  

이 이야기를 하면 꼭 영화 '엣지 오브 투모로우'가 떠오른다.  
주인공이 계속 죽으며 미레에 대한 정보를 기억하며 죽고 살아나기를 반복해 이를 바탕으로 계속 전투를 이겨나가고 결국 그 에피소드의 목적을 이뤄낸다.(오메가를 죽이는것)

#### 감가율
에이전트가 단지 동서남북 방향으로 움직이며 목적을 이루기 위해 행동한다고 하자.

에이전트에게 위로 가는 보상과 왼쪽으로 가는 보상이 같다면, 에이전트는 초기 상태에서 위로 갈지 왼쪽으로 갈지 판단할 수 없다.

즉 이 둘의 가중치가 같으면 어느 경로가 더 효율적인지 판단할 수 없다는 것이다.

이를 위해, 감가율(Discount Factor)을 도입한다.  
감가율은 통상적으로 $\gamma$ (Gamma)라고 표기한다.

0부터 1사이의 값으로 설정하며, 1에 가까울수록 미래의 보상에 더 많은 **가중치**를 두는 것을 의미한다.

즉, 우리가 목적지까지 가기 위해서 버스와 지하철 모두 타는 것이 가능하지만 무엇이 더 빠르게 목적지에 도달하는지에 대해 반영한다고 생각하면 된다.

보상에 감가율을 곱해가는 과정을 수식으로 표현하면 다음과 같다.  
$t$스텝에서의 반환 값은 일반적으로 $G_{t}$라고 표기한다.

$$ G_{t} = R_{t+1}+\gamma R_{t+2}+{\gamma}^2 R_{t+3}+{\gamma}^3 R_{t+4} + ... $$

감가율이 없으면  
>(1) 현재의 보상과 미래의 보상을 같게 취급하게 되고,  
(2) 100번의 보상을 1번 받는 것과 20번의 보상을 5번 받는 것을 구분할 수 없게 되고,  
(3) 시간이 무한대이면 보상이 발산하는 문제점이 발생함  

효율적인 경로로 이동하게 된다고 하더라도, 첫 에피소드에서 처음 찾은 루트에 대해서만 반환 값이 높은 상태로만 이동하게 되면 다른 루트는 찾을 수 없을 수도 있어서

가끔은 무작위로 움직여 여러 경로를 탐색할 필요도 있다.

### 정책(Policy)
정책은 특정 상태(state)에서 취할 수 있는 행동을 선택할 확률 분포를 출력해준다.

일반적으로 $\pi$ 로 표기하며, 아래와 같이 표현할 수 있다.

$$ \pi(a|s) = \begin{cases}
0.4, & \mbox{if }a\mbox{ = up} \\
0.4, & \mbox{if }a\mbox{ = left} \\
0.1, & \mbox{if }a\mbox{ = down} \\
0.1, & \mbox{if }a\mbox{ = right} 
\end{cases} $$

[다음 포스팅]()에서 계속해서 내용을 다루겠다.

📣<br> 
포스팅에서 오류나 궁금한 점은 Comments를 작성해주시면, 많은 도움이 됩니다.💡
{: .notice--info}