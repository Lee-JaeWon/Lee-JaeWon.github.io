---
title : "κ°€μΉ κΈ°λ° κ°•ν™”ν•™μµκ³Ό μ •μ±… κΈ°λ° κ°•ν™”ν•™μµ"
category :
    - Reinforcement_Learning
tag :
    - Reinforcement_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---

κ°•ν™”ν•™μµμ ν•™μµ λ°©λ²•μ„ μ•μ•„λ³΄μ.

---

κ°•ν™”ν•™μµμ ν•™μµ λ°©λ²•μ—λ” ν¬κ² κ°€μΉ κΈ°λ° κ°•ν™”ν•™μµκ³Ό μ •μ±… κΈ°λ° κ°•ν™”ν•™μµμ΄ μλ‹¤.

λ‘ ν•™μµμ€ μµμ μ μμ‚¬κ²°μ •μ„ μ„ν•΄ ν•™μµν•λ” λ€μƒμ— μ°¨μ΄κ°€ μλ‹¤. κ·Έ μ°¨μ΄λ¥Ό μ‚΄ν΄λ³΄μ.

### κ°€μΉ κΈ°λ° κ°•ν™”ν•™μµ
> Value based Reinforcement Learning

κ°€μΉ κΈ°λ° κ°•ν™” ν•™μµμ€ **κ°€μΉλ¥Ό ν† λ€λ΅ μμ‚¬κ²°μ •**μ„ ν•λ” κ²ƒμ΄λ‹¤.
μ¦‰, ν ν•¨μλ¥Ό ν•™μµν•μ—¬ [μµμ μ ν ν•¨μ](https://lee-jaewon.github.io/reinforcement_learning/RL_intro_2/#%EB%B2%A8%EB%A7%8C-%EB%B0%A9%EC%A0%95%EC%8B%9D)λ¥Ό μ–»κ³  μ΄λ¥Ό ν† λ€λ΅ μμ‚¬κ²°μ •μ„ ν•λ‹¤.

ν ν•¨μλ¥Ό μ—…λ°μ΄νΈν•λ” λ°©λ²•μ„ μ•μ•„λ³΄μ.  
μμ‹μ€ λ‹¤μκ³Ό κ°™λ‹¤.

$$ Q_{k+1}(S_{t},A_{t}) \longleftarrow Q_{k}(S_{t},A_{t}) + \alpha(R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') - Q_{k}(S_{t},A_{t})) $$

μ¦‰ $ Q_{k+1}(S_{t},A_{t}) $λ” κΈ°μ΅΄ $ Q_{k}(S_{t},A_{t}) $μ—μ„ $ \alpha(R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') - Q_{k}(S_{t},A_{t})) $λ¥Ό λ”ν• κ°’μΌλ΅ μƒλ΅ μ •μλλ‹¤.

$ \alpha $ λ” learning rateμ΄λ‹¤.

$ (R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') - Q_{k}(S_{t},A_{t})) $λ¥Ό μ‹κ°„μ°¨ μ¤μ°¨(Time-Difference Error)λΌκ³  ν•λ‹¤.

μ‹κ°„μ°¨ μ¤μ°¨λ” $t$ μ¤ν…μ—μ„ κ³„μ‚°ν• ν ν•¨μ κ°’κ³Ό μ‹¤μ  $t+1$ μ¤ν…μ—μ„ ν™•μΈν• ν ν•¨μ κ°’μ μ°¨μ΄λ¥Ό μλ―Έν•λ‹¤.

ν ν•¨μκ°€ μλ ΄ν–λ‹¤λ” μ΄μ•ΌκΈ°λ” κ²°κµ­ kλ² μ—…λ°μ΄νΈν• ν ν•¨μ $ Q_{k}(S_{t},A_{t}) $μ™€ $ Q_{k}(S_{t+1},A_{t+1}) $μ μ°¨μ΄κ°€ μ—†λ‹¤λ” λ»μ΄λ‹¤.  

μ¦‰ μ—ν”Όμ†λ“λ¥Ό μ§„ν–‰ν•λ©° μμΈ΅ν• $ Q_{k}(S_{t},A_{t}) $κ°’μ„ ν•™μµμ λ©ν‘μΈ $ R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') $κ°’μ— κ°€κΉμ›μ§€λ„λ΅ ν•λ” κ²ƒμ΄λ‹¤.

---

μ—…λ°μ΄νΈ ν•λ” κ³Όμ •μ€ μ΄μ  μ°λ¦¬κ°€ λ”¥λ¬λ‹μ—μ„ λ“¤μ–΄λ΄¤λ κ³Όμ •κ³Ό λΉ„μ·ν•λ‹¤.

μ†μ‹¤ ν•¨μ(Cost Function) $ L(\theta) $λ¥Ό μ •μν•΄μ¤€λ‹¤.  

$$ L(\theta)=E[(R_{t+1}+\gamma \underset{a'}{max} Q(S_{t+1},a';\theta) - Q(S_{t},A_{t};\theta))^2] $$

$ ;\theta $λ” ν ν•¨μκ°€ λ³€μ $\theta$λ΅ μ΄λ£¨μ–΄μ Έ μλ‹¤λ” κ²ƒμ„ μλ―Έν•λ©°,  
κ·Έλλ””μ–ΈνΈλ¥Ό μ‚¬μ©ν•΄ $L(\theta)$λ¥Ό μ¤„μ΄λ” λ°©ν–¥μΌλ΅ $\theta$λ¥Ό μ—…λ°μ΄νΈ ν•  κ²ƒμ΄λ‹¤.

μ΄λ¥Ό μ‹μΌλ΅ ν‘ν„ν•λ©΄ μ°λ¦¬μ—κ² λ°κ°€μ΄ μ‹μ΄ λ‚νƒ€λ‚λ‹¤.

$$ \theta_{k+1} \longleftarrow \theta_{k} - \alpha \cdot \bigtriangledown_{\theta} L_{\theta} $$

κΈ°μ΅΄μ $ \theta $ μ—μ„ μ†μ‹¤ν•¨μμ κΈ°μΈκΈ°μ— lrμ„ κ³±ν• κ²ƒμ„ λΉΌ λ‹¤μ‹ $ \theta $μ— assign ν•΄μ£Όλ”  
μ°λ¦¬κ°€ [λ”¥λ¬λ‹ κΈ°μ΄μ—μ„ Gradient Descent](https://lee-jaewon.github.io/deep_learning_study/Lec03(TensorFlow)/#how-itgd-works)μ—μ„ μ‚΄ν΄λ΄¤λ κ³Όμ •μ„μ„ ν™•μΈν•  μ μλ‹¤. 

---

μ„κΉμ§€ ν•΄μ„ μµμ μ ν ν•¨μλ¥Ό μ–»κΈ° μ„ν•΄ μ—…λ°μ΄νΈ ν•λ” λ°©λ²•μ„ μ‚΄ν΄λ΄¤λ‹¤.

μµμ μ ν ν•¨μλ¥Ό ν† λ€λ΅ ν•λ” μµμ μ μμ‚¬κ²°μ •, μ¦‰, μµμ μ μ •μ±…μ€ λ¬΄μ—‡μΌκΉ?

μµμ μ μ •μ±…μ„ μμ‹μΌλ΅ ν‘ν„ν•λ©΄ λ‹¤μκ³Ό κ°™λ‹¤.

$$ \pi_{*}(a|s) = \begin{cases}
1, & \mbox{if }a\mbox{ = $argmax_{a \in A}q_{*}(s,a)$} \\
0, & \mbox{ }\mbox{otherwise} 
\end{cases} $$

ν„μ¬ μƒνƒ sμ—μ„ ν–‰λ™ aλ¥Ό ν•  ν™•λ¥ μ€ κ°€μ¥ λ†’μ€ ν ν•¨μ κ°’μ„ μ–»λ” aμ— λ€ν•΄μ„λ§ 1μ΄κ³ , λ‚λ¨Έμ§€λ” 0μ΄λ‹¤.

μ΄λ° μ •μ±…μ„ κ²°μ •λ΅ μ (Deterministic)μ΄λΌκ³  ν‘ν„ν•λ©°  
μ΄κ²ƒμ€ μ •μ±…μ΄ κ°€λ¥ν• ν–‰λ™λ“¤μ— λ€ν• ν™•λ¥ μ„ μ¶λ ¥ν•λ” κ²ƒμ΄ μ•„λ‹ ν• κ°€μ§€ ν–‰λ™μ„ μ¶λ ¥ν•λ” κ²ƒμ„ μλ―Έν•λ‹¤.

μµμ μ κ°€μΉ ν•¨μλ¥Ό ν† λ€λ΅, μµμ μ μ •μ±…μ— λ”°λΌ ν–‰λ™μ„ κ³ λ¥΄λ” κ²ƒμ΄ κ°€μΉ κΈ°λ° κ°•ν™”ν•™μµμ΄λ‹¤.

κ°€μ¥ λ€ν‘μ μΈ μ•κ³ λ¦¬μ¦μΌλ΅λ” DQN(Deep Q-Network)μ΄ μλ‹¤.

### μ •μ±… κΈ°λ° κ°•ν™” ν•™μµ
> Policy based Reinforcement Learning

μ •μ±… κΈ°λ° κ°•ν™” ν•™μµμ€ κ°€μΉ κΈ°λ° κ°•ν™” ν•™μµκ³Όλ” λ‹¬λ¦¬ μμ‚¬ κ²°μ •μ— μμ–΄ κ°€μΉ ν•¨μλ¥Ό μ΄μ©ν•μ§€ μ•λ”λ‹¤.

κ°€μΉ ν•¨μκ°€ μ•„λ‹ μ •μ±…μ„ λ°”λ΅ ν•™μµν•μ—¬ ν•™μµν• μ •μ±…μ„ κ°€μ§€κ³  μμ‚¬κ²°μ •μ„ ν•λ‹¤.

μ •μ±…μ€ μ–΄λ–»κ² ν•™μµμ‹ν‚¬ μ μμ„κΉ?  
κ·Έκ²ƒμ€ μ •μ±… κ²½μ‚¬(Policy Gradient)λ¥Ό μ΄μ©ν•΄ ν•™μµν•λ‹¤.

μ •μ±…μ„ ν•™μµμ‹ν‚¤κΈ° μ„ν• λ©μ  ν•¨μ($J(\theta)$)λ” λ‹¤μκ³Ό κ°™λ‹¤.

$$ J(\theta) = v_{\pi_{\theta}}(S_{0}) $$

λ©μ  ν•¨μλ” μ •μ±… $ \pi_{\theta} $λ¥Ό ν†µν•΄ μμ‚¬ κ²°μ •μ„ ν–μ„ λ•, μ΄κΈ° μƒνƒμ κ°€μΉ ν•¨μλ¥Ό μλ―Έν•λ‹¤.  

ν• μ—ν”Όμ†λ“μ κ°€μΉλΌκ³  ν•  μ μλ” $ v_{\pi_{\theta}}(S_{0}) $ λ” μ •μ±… $ \pi_{\theta} $ κ°€ μµμ μ μ •μ±…μΌ μλ΅ λ†’μ•„μ§„λ‹¤.

λ”°λΌμ„ λ©μ  ν•¨μκ°€ μ μ  μ»¤μ§€λ” λ°©ν–¥μΌλ΅ $\theta$ λ¥Ό μ—…λ°μ΄νΈν•λ©΄ λλ‹¤. 

$$ \theta_{k+1} \longleftarrow \theta_{k} + \alpha \cdot \bigtriangledown_{\theta} J_{\theta} $$

μ •μ±… κΈ°λ° ν•™μµμ κ°€μ¥ λ€ν‘μ μΈ μ•κ³ λ¦¬μ¦μΌλ΅λ” REINFORCEκ°€ μλ‹¤.

π“£<br> 
ν¬μ¤ν…μ—μ„ μ¤λ¥λ‚ κ¶κΈν• μ μ€ Commentsλ¥Ό μ‘μ„±ν•΄μ£Όμ‹λ©΄, λ§μ€ λ„μ›€μ΄ λ©λ‹λ‹¤.π’΅
{: .notice--info}