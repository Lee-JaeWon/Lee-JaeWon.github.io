---
title : "가치 기반 강화학습과 정책 기반 강화학습"
category :
    - Reinforcement_Learning
tag :
    - Reinforcement_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---

강화학습의 학습 방법을 알아보자.

---

강화학습의 학습 방법에는 크게 가치 기반 강화학습과 정책 기반 강화학습이 있다.

두 학습은 최적의 의사결정을 위해 학습하는 대상에 차이가 있다. 그 차이를 살펴보자.

### 가치 기반 강화학습
> Value based Reinforcement Learning

가치 기반 강화 학습은 **가치를 토대로 의사결정**을 하는 것이다.
즉, 큐 함수를 학습하여 [최적의 큐 함수](https://lee-jaewon.github.io/reinforcement_learning/RL_intro_2/#%EB%B2%A8%EB%A7%8C-%EB%B0%A9%EC%A0%95%EC%8B%9D)를 얻고 이를 토대로 의사결정을 한다.

큐 함수를 업데이트하는 방법을 알아보자.  
수식은 다음과 같다.

$$ Q_{k+1}(S_{t},A_{t}) \longleftarrow Q_{k}(S_{t},A_{t}) + \alpha(R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') - Q_{k}(S_{t},A_{t})) $$

즉 $ Q_{k+1}(S_{t},A_{t}) $는 기존 $ Q_{k}(S_{t},A_{t}) $에서 $ \alpha(R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') - Q_{k}(S_{t},A_{t})) $를 더한 값으로 새로 정의된다.

$ \alpha $ 는 learning rate이다.

$ (R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') - Q_{k}(S_{t},A_{t})) $를 시간차 오차(Time-Difference Error)라고 한다.

시간차 오차는 $t$ 스텝에서 계산한 큐 함수 값과 실제 $t+1$ 스텝에서 확인한 큐 함수 값의 차이를 의미한다.

큐 함수가 수렴했다는 이야기는 결국 k번 업데이트한 큐 함수 $ Q_{k}(S_{t},A_{t}) $와 $ Q_{k}(S_{t+1},A_{t+1}) $의 차이가 없다는 뜻이다.  

즉 에피소드를 진행하며 예측한 $ Q_{k}(S_{t},A_{t}) $값을 학습의 목표인 $ R_{t+1}+\gamma \underset{a'}{max} Q_{k}(S_{t+1},a') $값에 가까워지도록 하는 것이다.

---

업데이트 하는 과정은 이제 우리가 딥러닝에서 들어봤던 과정과 비슷하다.

손실 함수(Cost Function) $ L(\theta) $를 정의해준다.  

$$ L(\theta)=E[(R_{t+1}+\gamma \underset{a'}{max} Q(S_{t+1},a';\theta) - Q(S_{t},A_{t};\theta))^2] $$

$ ;\theta $는 큐 함수가 변수 $\theta$로 이루어져 있다는 것을 의미하며,  
그래디언트를 사용해 $L(\theta)$를 줄이는 방향으로 $\theta$를 업데이트 할 것이다.

이를 식으로 표현하면 우리에게 반가운 식이 나타난다.

$$ \theta_{k+1} \longleftarrow \theta_{k} - \alpha \cdot \bigtriangledown_{\theta} L_{\theta} $$

기존의 $ \theta $ 에서 손실함수의 기울기에 lr을 곱한 것을 빼 다시 $ \theta $에 assign 해주는  
우리가 [딥러닝 기초에서 Gradient Descent](https://lee-jaewon.github.io/deep_learning_study/Lec03(TensorFlow)/#how-itgd-works)에서 살펴봤던 과정임을 확인할 수 있다. 

---

위까지 해서 최적의 큐 함수를 얻기 위해 업데이트 하는 방법을 살펴봤다.

최적의 큐 함수를 토대로 하는 최적의 의사결정, 즉, 최적의 정책은 무엇일까?

최적의 정책을 수식으로 표현하면 다음과 같다.

$$ \pi_{*}(a|s) = \begin{cases}
1, & \mbox{if }a\mbox{ = $argmax_{a \in A}q_{*}(s,a)$} \\
0, & \mbox{ }\mbox{otherwise} 
\end{cases} $$

현재 상태 s에서 행동 a를 할 확률은 가장 높은 큐 함수 값을 얻는 a에 대해서만 1이고, 나머지는 0이다.

이런 정책을 결정론적(Deterministic)이라고 표현하며  
이것은 정책이 가능한 행동들에 대한 확률을 출력하는 것이 아닌 한 가지 행동을 출력하는 것을 의미한다.

최적의 가치 함수를 토대로, 최적의 정책에 따라 행동을 고르는 것이 가치 기반 강화학습이다.

가장 대표적인 알고리즘으로는 DQN(Deep Q-Network)이 있다.

### 정책 기반 강화 학습
> Policy based Reinforcement Learning

정책 기반 강화 학습은 가치 기반 강화 학습과는 달리 의사 결정에 있어 가치 함수를 이용하지 않는다.

가치 함수가 아닌 정책을 바로 학습하여 학습한 정책을 가지고 의사결정을 한다.

정책은 어떻게 학습시킬 수 있을까?  
그것은 정책 경사(Policy Gradient)를 이용해 학습한다.

정책을 학습시키기 위한 목적 함수($J(\theta)$)는 다음과 같다.

$$ J(\theta) = v_{\pi_{\theta}}(S_{0}) $$

목적 함수는 정책 $ \pi_{\theta} $를 통해 의사 결정을 했을 때, 초기 상태의 가치 함수를 의미한다.  

한 에피소드의 가치라고 할 수 있는 $ v_{\pi_{\theta}}(S_{0}) $ 는 정책 $ \pi_{\theta} $ 가 최적의 정책일 수록 높아진다.

따라서 목적 함수가 점점 커지는 방향으로 $\theta$ 를 업데이트하면 된다. 

$$ \theta_{k+1} \longleftarrow \theta_{k} + \alpha \cdot \bigtriangledown_{\theta} J_{\theta} $$

정책 기반 학습의 가장 대표적인 알고리즘으로는 REINFORCE가 있다.

📣<br> 
포스팅에서 오류나 궁금한 점은 Comments를 작성해주시면, 많은 도움이 됩니다.💡
{: .notice--info}