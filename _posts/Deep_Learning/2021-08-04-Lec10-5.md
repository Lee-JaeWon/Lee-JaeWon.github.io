---
title : "Lec 10-5: Advanced CNN"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Deep_Learning study Lec10-5

## Boostcourse의 '파이토치로 시작하는 딥러닝 기초'를 통한 공부와 정리 Post    

## Goal of Study  
> - 유명한 CNN 모델들에 대해 알아본다.  

### Keyword
> - VGG    
> - CNN  
    
## 1. 강의 요약  

CNN에 관해 더 정확도를 높이고, 효율적으로 학습하기 위해 다양한 방법들이 연구되었다.

그래서 Advanced CNN에 대해 어떠한 모델들이 있었는지 간단하게 살펴보도록 하겠다.

### AlexNet
<img src="https://user-images.githubusercontent.com/72693388/128127371-5d433968-881e-4e96-8c5d-b63ef3813b20.png" width = "300" >
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/128127286-c4bdac6f-0274-4235-93fb-2d89f9a29af9.png" width = "400" ></p>

AlexNet은 2012년에 개최된 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회에서 12년에 우승을 하였고,  

CNN의 부흥에 큰 역할을 한 구조라 할 수 있다.

Layer 형태는 다음과 같다.
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/128127535-39887983-fd51-48f5-abe0-91de332afe9c.png" width = "400" ></p>

LeNet(우리가 다루었던 기본개념과 비슷함)과 크게 다르지 않다, 당시 프로세서가 좋지 않아 GPU 2개로 병렬연산을 수행하기 위한 구조로 설계되었다는 특징이 있다.

레이어마다의 특성을 살펴보고 싶다면, [AlexNet의 구조](https://bskyvision.com/421)에 관해 다른 분이 굉장히 잘 포스팅 해놓으셔서 관심있다면 참고해도 좋을 듯하다.

이미 CNN을 다루어보고 학습시켜봤다면, 비슷한 내용인 것을 알 수 있을 것이다.

AlexNet의 특징은 아래와 같다.

> - First use of ReLU  
> - Norm Layer를 사용  
> - Heavy Data Argumentation  
> - Dropout : 0.5  
> - Batch size : 128  
> - SGD Momentum : 0.9  
> - Learning rate : 1e-2  
> - L2 weight decay : 5e-4   
> - 7 CNN ensemble : 18.2% -> 15.4%  

### GoogLeNet
GoogLeNet은 아래와 같이 상당히 복잡한 구조로 이루어져있다.
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/128133863-aa734030-b246-4524-ae38-343ae613964a.png" width = "500" ></p>

가장 큰 특징으로는 Inception Module을 사용했다는 점이다.
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/128135295-3e4ddab5-7b42-4cca-bca2-6a65e18429a4.png" width = "400" ></p>

### ResNet
그리고 이제 이에 이어서, 15년에 ResNet이라는 강력한 Architecture가 나왔다.

에러가 3.6%로 낮아지며 사람을 능가하는 능력치를 보이며 많은 대회를 휩쓸었다.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/128135698-014c7733-7ee7-4540-a47f-f4b9cd713cc8.png" width = "500" ></p>
레이어가 이전에 비해 굉장히 deep해진것을 확인할 수 있는데,

이 때, 우리의 생각으로는 학습하는데 너무 오래걸리거나 원활히 되지 않을 것같다는 생각을 하게 된다.

그럼 이를 어떻게 해결했을까?

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/128136690-03e78aa0-af8d-4a23-885a-9048546d9098.png" width = "500" ></p>

Forward과정에서 다음과 같이 건너뛰는 방식으로 진행하면서 효율적으로 문제를 해결했다고 한다.

즉, 레이어를 쌓아 올리는 것이 아닌 이를 건너뛰는 skip connection을 통해 학습을 진행하는 것이다.

강의에서는 간단히 설명해서 다른 포스팅에서 ResNet에 관한 내용을 조금 더 심층있게 다루겠다.

## VGGnet
> Oxford VGG(visual Geometry Group)에서 만든 Network  
14년도에 GoogLeNet과 함께 주목을 받았으며, 간소한 차위로 2위를 차지했다.

VGGnet은 network 깊이가 어떤 영향을 주는지 연구하기 위해 설계된 network이다.

특징으로는,  
3 by 3 convolutional 필터만을 적용하여 간단한 구성을 보이지만, AlexNet에 비해 Layer가 더 깊어졌다.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/128138692-d41a1f35-700a-4444-be86-178a4c930823.png" width = "400" ></p>

11 layer 부터 19 layer까지 다양하게 있으며,

Pytorch에서 VGG11~VGG19 function으로 제공한다.

이를 실제 코드와 함께 [다음 포스팅]()에서 집중적으로 다루도록 하겠다.

📣<br>
본 포스팅의 학습 환경 : `Anaconda`, `CPU`, `Pytorch`, `Jupyter Notebook`  
포스팅에서 오류나 궁금한 점은 Comments를 작성해주시면, 많은 도움이 됩니다.💡  
{: .notice--info}