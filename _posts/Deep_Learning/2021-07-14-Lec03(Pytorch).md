---
title : "Deep_Learning study Lec03 (Pytorch)"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Lec 03: Deeper Look at GD

## Boostcourse의 '파이토치로 시작하는 딥러닝 기초'를 통한 공부와 정리 Post

## Goal of Study
> - 경사하강법(Gradient descent)에 대해 더 자세히 알아본다.   

### Keyword
> - 가설 함수(Hypothesis Function)  
> - 평균 제곱 오차(Mean Squared Error)  
> - 경사하강법(Gradient descent)  
    
'파이토치로 시작하는 딥러닝 기초'와 '텐서플로우로 시작하는 딥러닝 기초'를 함께 공부하며 개념적인 부분에서 상호보완하고 있기 때문에 다른 포스트에서 같은 내용을 다루는 부분이 있을 수 있습니다.  

이번 포스팅은 [이전 Lec03(TensorFlow) 포스팅](https://lee-jaewon.github.io/deep_learning_study/Lec03(TensorFlow)/)에 이어서 개념의 보강과 함께 Pytorch 실습을 다룹니다.

## 1. 강의 요약  
### Hypothesis ++
Hypothesis와 Linear Regression에 대한 이야기를 다루었었다.

다시 W와 b의 개념을 추가하며 복기해보자면, Hypothesis function은 input x에 대해 output y를 예측해준다.  

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/125319815-11403a00-e376-11eb-8652-e426ba02a31b.png" width = "300" ></p>
여기서 W(Weight)는 하나의 Matrix이고 이것이 input 벡터와 곱해진 후, b(bias) 벡터에 더해져서 예측 값을 계산한다.

이 때, W와 b라는 변수를 학습하여 주어진 데이터에 최적화 하는 것이다.  
Linear Regression 에서는 Cost를 최소화 하기 위한 작업을 한다.   
(공학의 문제의 대부분은 최적화에 관한 문제이다.)

## 2. 실습
[SGD 실습 포스팅](https://lee-jaewon.github.io/deep_learning_study/Lec02/#4-%EB%AA%A8%EB%8D%B8-%EA%B3%84%EC%84%A0-%EB%B0%8F-gradient-descent%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95)에서 사용하였던 optim 라이브러리와 SGD를  
[이전 포스팅(Gradient Descent)](https://lee-jaewon.github.io/deep_learning_study/Lec03(TensorFlow)/)에서 다뤘던 개념을 통해 다시 실습해보고 마무리하겠습니다.

```python
import torch

x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])

W = torch.zeros(1)

lr = 0.1

nb_epochs = 10
for epoch in range(1, nb_epochs + 1):
    
    # H(x)
    hypothesis = x_train * W
     
    #cost gradient
    cost = torch.mean((hypothesis - y_train) ** 2)
    gradient = torch.sum((W * x_train - y_train) * x_train)

    print('Epoch {:4d}/{}  W: {:.3f}, Cost: {".6f}'.format(epoch, nb_epochs, W.item(), cost.item()))

    W -= lr * gradient

```
위의 과정을 torch.optim에서 간단히 했었지만 개념을 통해 위와 같은 코드로도 표현할 수 있다.

결과는 다음과 같다.
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/125634502-464a4e08-b998-4890-856a-4ee842bf0d3c.png" width = "700" ></p>

cost는 0이 되었고(Cost의 Minimize), W도 1이 된 것을 확인할 수 있다.  

W = 1 인 지점은 cost function에서 미분계수가 0인 지점이다.

즉, Cost를 최소화하는 W를 찾아가는 과정  
이것이 머신 러닝의 핵심인 학습 과정이라고 할 수 있다.