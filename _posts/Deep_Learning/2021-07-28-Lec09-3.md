---
title : "Lec 09-3: Dropout"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Deep_Learning study Lec09-3 

## Boostcourseì˜ 'íŒŒì´í† ì¹˜ë¡œ ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ì´ˆ'ë¥¼ í†µí•œ ê³µë¶€ì™€ ì •ë¦¬ Post    

## Goal of Study  
> - ë“œë¡­ì•„ì›ƒ(Dropout) ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.  

### Keyword
> - ê³¼ìµœì í™”(Overfitting)     
> - ë“œë¡­ì•„ì›ƒ(Dropout)       

## 1. ê°•ì˜ ìš”ì•½  
### Overfitting
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127322339-6d435f0a-2ff0-4179-9102-e3c341cd897e.png" width = "500" ></p>  

ê³„ì†í•´ì„œ ë°ì´í„°ë¥¼ ì˜ ëŒ€ë³€í•˜ëŠ” ëª¨ë¸ì„ ì°¾ëŠ”ê²ƒì´ ìš°ë¦¬ì˜ ëª©ì ì´ë‹¤.  

ì‚¬ëŒì— ë”°ë¼ ì–´ë–¤ê²Œ ì˜ fitting ëœ ê²ƒì¸ì§€ëŠ” ì£¼ê´€ì ì¸ ì°¨ì´ê°€ ìˆê² ì§€ë§Œ, ê¸°ê³„ê°€ ì´ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆëŠ”ë°,  
ì´ì— ë”°ë¼, Underfittingë¬ëŠ”ì§€ Overfitting ë¬ëŠ”ì§€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆë‹¤.

Underfittingì˜ ê²½ìš°ì—ëŠ” ë‹¹ì—°í•˜ê²Œ ë°ì´í„°ë¥¼ ì˜ ëŒ€ë³€í•˜ê¸° ìœ„í•œ í•™ìŠµì´ ëœ ë˜ê±°ë‚˜, ë„ˆë¬´ ë‚®ì€ ì°¨ì›ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤ëŠ” ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤.

Overfittingì˜ ê²½ìš°ì—ëŠ” ë„ˆë¬´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ fittingì´ ë˜ì–´ìˆê³ , ë„ˆë¬´ ë†’ì€ ì°¨ì›ì˜ ëª¨ë¸ì„ ì‚¬ìš©í–ˆë‹¤ëŠ” ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤.

> - ì™œ Overfittingì´ ë¬¸ì  ì§€ ì•Œì•„ë³´ì.

ë§Œì•½ Classifiacationì— ê´€í•œ ë¬¸ì œì—ì„œ Overfittingì„ í•  ê²½ìš° Trainì¤‘ì—ëŠ” Accuracyê°€ 100%ì— ë„ë‹¬í•˜ê² ì§€ë§Œ,  
ì´ë¥¼ Testí•  ì ì—ëŠ” ì˜¤íˆë ¤ ì˜¤ë¶„ë¥˜ ë˜ê±°ë‚˜ ì •í™•í•˜ì§€ ì•ŠëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127323388-2327567b-e889-4c73-b6f0-80d1d0115628.png" width = "400" ></p>  

ê·¸ ì´ìœ ëŠ” Train-setê³¼ Test-Setì´ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ê³ , ì´ëŸ¬í•œ ê²½ìš°ê°€ ì¼ë°˜ì ì´ë‹¤.  

ê·¸ë ‡ê¸°ì— Overfitting, ê³¼ìµœì í™”ëœ ëª¨ë¸ì€ ì˜¤íˆë ¤ Errorìœ¨ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ë¬¸ì œì ì„ ì§€ë‹ˆê³  ìˆë‹¤.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127324204-765085ff-3405-4b66-9176-afb4eda55f56.png" width = "400" ></p>

Overfittingì— ëŒ€í•œ í•´ê²°ì±…ìœ¼ë¡œ Training dataë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, íŠ¹ì§•ì„ ì¤„ì´ê±°ë‚˜, ì •ê·œí™”ë¥¼ í•œë‹¤ ë“± ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆë‹¤.

ê·¸ ì¤‘, Dropoutì´ë¼ëŠ” ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë³´ì.

### Dropout
ë“œë¡­ì•„ì›ƒì€ í•™ìŠµ ê³¼ì •ì—ì„œ ì‹ ê²½ë§ì˜ ì¼ë¶€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë‹¤.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127331079-65a6d559-28e3-486d-b2fa-afdfa3b2ed7b.png" width = "400" ></p>

ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨ì„ ì •í•´ì£¼ëŠ”ë§Œí¼ **í•™ìŠµê³¼ì • ë§ˆë‹¤** ëœë¤ìœ¼ë¡œ ê·¸ ë¹„ìœ¨ë§Œí¼ì˜ ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë‚˜ë¨¸ì§€ ë‰´ëŸ°ë§Œì„ ì‚¬ìš©í•œë‹¤.

ë§Œì•½ ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨ì„ 0.5ë¡œ í•œë‹¤ë©´ ì ˆë°˜ì€ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê²Œ ëœë‹¤.  
(í•™ìŠµ ê³¼ì •(step)ë§ˆë‹¤ ì¸ ê²ƒì— ìœ ì˜í•œë‹¤.)  

ì´ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ì‹¤ì§ˆì ìœ¼ë¡œ Overfittingì„ ë°©ì§€í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ë¥¼ ì–»ê²Œ ë˜ë©° ì„±ëŠ¥ì˜ í–¥ìƒì´ ì¼ì–´ë‚œë‹¤.

í•™ìŠµ ì‹œì— ì¸ê³µ ì‹ ê²½ë§ì´ íŠ¹ì • ë‰´ëŸ° ë˜ëŠ” íŠ¹ì • ì¡°í•©ì— ë„ˆë¬´ ì˜ì¡´ì ì´ê²Œ ë˜ëŠ” ê²ƒì„ ë°©ì§€í•´ì£¼ë©°   
ë§¤ë²ˆ ë‹¤ë¥¸ í˜•íƒœì˜ Network êµ¬ì¡°ë“¤ì´ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë‚´ê¸° ë•Œë¬¸ì—, ì„œë¡œ ë‹¤ë¥¸ ì‹ ê²½ë§ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ì€ íš¨ê³¼ë¥¼ ë‚´ì–´ Overfittingì„ ë°©ì§€í•œë‹¤.

### mnist_nn_dropout
[ì´ì „ í¬ìŠ¤íŒ…_Xavier,deep](https://lee-jaewon.github.io/deep_learning_study/Lec09-2/#mnist_nn_deep)ì—ì„œì™€ì˜ ì°¨ì´ì ì„ ë¨¼ì € í™•ì¸í•˜ê³  ì½”ë“œë¥¼ ë³´ê¸°ë¥¼ ê¶Œì¥í•œë‹¤.

Dropoutì„ ì“°ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ê³¼ì •ì´ ì¶”ê°€ë˜ì—ˆë‹¤.  
íŠ¹íˆ, Dropoutì˜ ë¹„ìœ¨ì„ ì •í•´ì£¼ì—ˆê³ , dropoutì€ trainí•  ë•Œë§Œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì—

Train & eval ëª¨ë“œë¥¼ í•¨ìˆ˜ë¥¼ í†µí•´ trainì‹œì—ë§Œ dropoutì´ Trueì´ë„ë¡ í•˜ì˜€ë‹¤.

```python
#ì¶”ê°€ ê³¼ì •(... ì€ ì¤‘ëµ)
#dropout ë¹„ìœ¨
drop_prob = 0.3

...

relu = torch.nn.ReLU()
dropout = torch.nn.Dropout(p=drop_prob)

...

# model
model = torch.nn.Sequential(linear1, relu, dropout,
                            linear2, relu, dropout,
                            linear3, relu, dropout,
                            linear4, relu, dropout,
                            linear5).to(device)

...

total_batch = len(data_loader)
model.train()    # set the model to train mode (dropout=True)
for epoch in range(training_epochs):
    avg_cost = 0

...

with torch.no_grad():
    model.eval()    # set the model to evaluation mode (dropout=False)

```

#### Full code
```python
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import random
```


```python
USE_CUDA = torch.cuda.is_available() # GPUë¥¼ ì‚¬ìš©ê°€ëŠ¥í•˜ë©´ True, ì•„ë‹ˆë¼ë©´ Falseë¥¼ ë¦¬í„´
device = torch.device("cuda" if USE_CUDA else "cpu") # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©í•˜ê³  ì•„ë‹ˆë©´ CPU ì‚¬ìš©
print("ë‹¤ìŒ ê¸°ê¸°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤:", device)

# for reproducibility
random.seed(777)
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
```

    ë‹¤ìŒ ê¸°ê¸°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤: cpu
    


```python
# parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100
drop_prob = 0.3

# MNIST dataset
mnist_train = dsets.MNIST(root='E:\DeepLearningStudy',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

mnist_test = dsets.MNIST(root='E:\DeepLearningStudy',
                         train=False,
                         transform=transforms.ToTensor(),
                         download=True)

# dataset loader
data_loader = torch.utils.data.DataLoader(dataset=mnist_train,
                                          batch_size=batch_size,
                                          shuffle=True,
                                          drop_last=True)
```

    C:\Users\LeeJaeWon\anaconda3\envs\DeepLearningStudy\lib\site-packages\torchvision\datasets\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\torch\csrc\utils\tensor_numpy.cpp:180.)
      return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
    


```python
# nn layers
linear1 = torch.nn.Linear(784, 512, bias=True)
linear2 = torch.nn.Linear(512, 512, bias=True)
linear3 = torch.nn.Linear(512, 512, bias=True)
linear4 = torch.nn.Linear(512, 512, bias=True)
linear5 = torch.nn.Linear(512, 10, bias=True)
relu = torch.nn.ReLU()
dropout = torch.nn.Dropout(p = drop_prob)
```


```python
# xavier initialization
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)
torch.nn.init.xavier_uniform_(linear4.weight)
torch.nn.init.xavier_uniform_(linear5.weight)
```




    Parameter containing:
    tensor([[-0.0565,  0.0423, -0.0155,  ...,  0.1012,  0.0459, -0.0191],
            [ 0.0772,  0.0452, -0.0638,  ...,  0.0476, -0.0638,  0.0528],
            [ 0.0311, -0.1023, -0.0701,  ...,  0.0412, -0.1004,  0.0738],
            ...,
            [ 0.0334,  0.0187, -0.1021,  ...,  0.0280, -0.0583, -0.1018],
            [-0.0506, -0.0939, -0.0467,  ..., -0.0554, -0.0325,  0.0640],
            [-0.0183, -0.0123,  0.1025,  ..., -0.0214,  0.0220, -0.0741]],
           requires_grad=True)




```python
# model
model = torch.nn.Sequential(linear1, relu, dropout,
                            linear2, relu, dropout,
                            linear3, relu, dropout,
                            linear4, relu, dropout,
                            linear5).to(device)
```


```python
# define cost/loss & optimizer
criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```


```python
total_batch = len(data_loader)
model.train()    # set the model to train mode (dropout=True)
for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = len(data_loader)

    for X, Y in data_loader:
        # reshape input image into [batch_size by 784]
        # label is not one-hot encoded
        X = X.view(-1, 28 * 28).to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))

print('Learning finished')
```

    Epoch: 0001 cost = 0.311706781
    Epoch: 0002 cost = 0.146726266
    Epoch: 0003 cost = 0.112394512
    Epoch: 0004 cost = 0.092713356
    Epoch: 0005 cost = 0.084646218
    Epoch: 0006 cost = 0.072477818
    Epoch: 0007 cost = 0.067062818
    Epoch: 0008 cost = 0.060241420
    Epoch: 0009 cost = 0.060738977
    Epoch: 0010 cost = 0.054180898
    Epoch: 0011 cost = 0.050488908
    Epoch: 0012 cost = 0.049583405
    Epoch: 0013 cost = 0.047045611
    Epoch: 0014 cost = 0.045497715
    Epoch: 0015 cost = 0.045529798
    Learning finished
    


```python
# Test the model using test sets
with torch.no_grad():
    model.eval()    # set the model to evaluation mode (dropout=False)
    
    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = model(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy:', accuracy.item())

    # Get one and predict
    r = random.randint(0, len(mnist_test) - 1)
    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)
    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)

    print('Label: ', Y_single_data.item())
    single_prediction = model(X_single_data)
    print('Prediction: ', torch.argmax(single_prediction, 1).item())
    
    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')
    plt.show()
```

    Accuracy: 0.9797999858856201
    Label:  5
    Prediction:  5

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127334837-acb5f68c-8409-4542-a4f1-ee1f37b82720.png" width = "400" ></p>

ğŸ“£<br>
ë³¸ í¬ìŠ¤íŒ…ì˜ í•™ìŠµ í™˜ê²½ : `Anaconda`, `CPU`, `Pytorch`, `Jupyter Notebook`  
í¬ìŠ¤íŒ…ì—ì„œ ì˜¤ë¥˜ë‚˜ ê¶ê¸ˆí•œ ì ì€ Commentsë¥¼ ì‘ì„±í•´ì£¼ì‹œë©´, ë§ì€ ë„ì›€ì´ ë©ë‹ˆë‹¤.ğŸ’¡
{: .notice--info}