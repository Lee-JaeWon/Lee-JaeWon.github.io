---
title : "[논문 리뷰] KPConv: Flexible and Deformable Convolution for Point Clouds"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---
KPConv: Flexible and Deformable Convolution for Point Clouds 리뷰

- 3D Point Cloud 딥러닝에 관한 이전 포스팅들
<br>
[PointNet](https://lee-jaewon.github.io/deep_learning_study/pointnet/)
<br>
[PointNet++](https://lee-jaewon.github.io/deep_learning_study/pointnet_pp/)<br>
[DGCNN](https://lee-jaewon.github.io/deep_learning_study/DGCNN/)

<br>

# Introduction

(본론은 [**여기**](https://lee-jaewon.github.io/deep_learning_study/kpconv/#kernel-point-convolution)부터 보셔도 됩니다.)<br><br>

깊은 학습의 도래로 현대 컴퓨터 비전은 discrete convolution을 기본 구성 요소로 사용하여 크게 발전되었다.<br><br>

이 연산은 2D 그리드 상의 지역 이웃 데이터를 결합한다.<br><br>

이러한 정규 구조 덕분에 현대 하드웨어에서 높은 효율성으로 계산될 수 있지만, 이러한 정규 구조를 상실하면 2D 그리드 상에서와 같은 효율성으로 합성곱 연산을 올바르게 정의하기 어렵다.<br><br>

3D 스캐닝 기술의 발전으로 인해 이러한 정규 그리드가 아닌 데이터에 의존하는 여러 응용 프로그램이 발전해왔다.<br><br>

예를 들어, 3D 포인트 클라우드 segmentation이나 3D SLAM은 non-grid 구조 데이터인 포인트 클라우드에 의존합니다.<br><br>

포인트 클라우드는 3D(또는 고차원) 공간의 점들로 구성된 집합입니다. 많은 응용 프로그램에서는 이 점들이 색상과 같은 해당 특성과 결합된다.<br><br>

본 논문에서 포인트 클라우드는 이러한 두 가지 요소, 즉 점 $P \in \mathbb{R}^{N \times 3}$ (3차원 공간에서의 좌표)와 특성(feature) $F \in \mathbb{R}^{N \times D}$ 를 고려한다.<br><br>

포인트 클라우드는 순서가 없는 sparse한 구조이므로 그리드와는 매우 다르다. 그러나 합성곱의 정의에 필수적인 공통된 속성이 있다. 바로 공간적으로 지역화(spatially localized)되어 있다는 점이다.<br><br>

그리드에서는 특성들은 행렬 내에서 인덱스로 지역화되지만 포인트 클라우드에서는 해당 점 좌표에 따라 지역화된다.<br><br>

따라서 점들은 구조적 요소로 간주되고 특성들은 실제 데이터로 간주되어야 한다.<br><br>

여러 접근 방법이 제안되었으며,<br>
몇 가지 방법은 그리드 기반 범주에 속하며, 이 방법들은 희소한 3D 데이터를 더 쉽게 다룰 수 있는 정규 구조로 데이터를 투영하거나<br>

다른 접근 방법은 다층 퍼셉트론(MLP)을 사용하여 포인트 클라우드를 직접 처리한다.<br><br>

더 최근에는 몇 가지 시도가 있어 합성곱을 직접 포인트에 적용하는 방법들이 등장했다. 이러한 방법들은 포인트 클라우드의 공간적 지역화 속성을 활용하여 공간적 커널을 가진 포인트 합성곱을 정의한다.<br><br>

---

본 논문에서는 새로운 포인트 합성곱 연산자인 **Kernal Point Convolution (KPConv)**을 소개한다.<br><br>
KPConv는 로컬 3D 필터들의 집합으로 구성되지만 관련 작업에서 보여준 이전 포인트 합성곱의 한계를 극복한다.<br><br>
KPConv는 이미지 기반 합성곱에서 영감을 받았지만, 커널 픽셀 대신에 각 커널 가중치가 적용되는 영역을 정의하는 커널 포인트 집합을 사용한다.<br><br>

<p align="center"><img src="/MyPDF/kpconv(1).png" width = "500" ></p>
<p align="center"> Figure 1. </p>

kernel weight는 input feature와 같이 포인트에 의해 운반되며(carried by points라고 되어있는데 정확히 무슨말인지는 조금 더 봐야할 것 같다.),<br>
그들의 영향영역(area of influenece)는 correlation function에 의해 정의된다.<br><br>

커널 포인트의 개수는 제약이 없어서 설계가 매우 유연하다.<br><br>

이전 연구들과 다르게 커널 포인트를 사용하지만 커널에 대한 가중치 없이 지역 기하 패턴을 학습한다.<br><br>

---
더 나아가, 또한 변형 가능한 버전의 합성곱을 제안한다.<br>
<p align="center"><img src="/MyPDF/kpconv(2).png" width = "500" ></p>
<p align="center"> Figure 3. </p>

이것은 커널 포인트에 적용되는 **local shift**를 학습하는 것으로, 각 합성곱 위치마다 다른 shift를 생성하여 입력 클라우드의 다양한 영역에 대해 커널의 형태를 조절할 수 있게 한다.<br><br>

**Deformable convolution**은 이미지의 변형 가능한 합성곱과는 다른 방식으로 설계되었다.<br>
데이터의 다른 특성으로 인해 데포메이션 커널이 포인트 클라우드 기하학에 잘 맞도록 도와주는 정규화가 필요하다.<br>우리는 효과적인 수용 영역(ERF)[22]과 제거 실험을 사용하여 Rigid KPConv와 Deformable KPConv를 비교한다.(Ablation study)<br><br>

---
또한 몇 개의 이전 연구들과 달리 k-최근접 이웃(KNN) 대신 반경 이웃(**radius neighborhoods**)을 선호한다.<br><br>

[13](https://arxiv.org/abs/1806.01759)에서 보이듯이, **KNN은 non-uniform sampling 설정에서 robust하지 않다.**<br><br>

반경 이웃과 입력 클라우드의 정규 하위 샘플링(regular subsampling)의 조합에 의해 본 논문의 합성곱은 변동되는 밀도에 대해 견고성을 보장한다.<br><br>

이전 연구들의 정규화 전략과 비교해 본 논문의 접근 방법은 합성곱의 계산 비용도 줄인다.<br><br>

(.. 이 뒤 내용은 실험 및 성능 향상, 우수성에 관한 이야기라 pass)

# Related Work

- Projection networks<br>
몇 가지 방법은 포인트를 중간 그리드 구조로 투영<br>
혹은 Voxel 기반 방법<br><br>
- Graph convolution networks<br><br>
- Pointwise MLP networks<br>
PointNet<br><br>
- Point convolution networks<br>
Pointwise CNN<br>
SpiderCNN<br>
Flex-convolution<br>
PCNN<br><br>

# Kernel Point Convolution
## A Kernel Function Defined by Points
이전 연구들과 마찬가지로 KPConv는 일반적인 포인트 합성곱의 정의로 표현할 수 있으며, 이미지 합성곱에서 영감을 받았다.<br><br>

명확성을 위해,<br>
$\mathcal{P} \in \mathbb{R}^{N \times 3}$ 의 점 $x_i$와 이에 해당하는 feature인 $\mathcal{F} \in \mathbb{R}^{N \times D}$ 의 $f_i$로 정의한다.<br><br>

$(\mathcal{F} *g)(x)=\underset{x_i \in \mathcal{N}_{x}}{\sum} = g(x_i-x)f_i$

<br><br>

<p align="center"><img src="/MyPDF/kpconv(3).png" width = "700" ></p>
<p align="center"> Figure 2. </p>

radius neighborhood를 이용하여 변동되는 밀도에 대해 강인함을 보장한다.<br><br>

따라서 

$\mathcal{N}_{x} = \{ x_i \in \mathcal{P} \ | \ \Vert x_i-x \Vert <= r  \} \ \ \cdots \ \  (1)$

<br><br>

$r \in \mathbb{R}$ 은 선택한 반경이다.<br><br>

Intro에서 언급했듯이 반경 이웃을 사용하여 계산된 손으로 제작된 3D 포인트 특성이 KNN보다 더 나은 표현을 제공한다.<br><br>

본 논문은 함수 g에 일관된 구 형 도메인이 있으면 네트워크가 의미 있는 표현을 학습하는 데 도움이 될 것으로 믿는다.<br><br>

식 (1)의 중요한 부분은 **커널 함수 g**의 정의이며, 이것이 KPConv의 특징이다. g는 x를 중심으로 한 이웃 위치를 **입력으로 사용**한다.<br>
(이웃 : $y_i=x_i-x$)<br>
<br>

g는 $\mathcal{B}_{r}^{3} = \{ y \in \mathbb{R}^{3} | \ \Vert y \Vert <= r \}$ 범위로 정의된다.<br><br>

이미지 합성곱 커널과 유사하게 (이미지 합성곱과 KPConv 사이의 자세한 비교는 Figure 2에서 확인할 수 있다), 본 논문은 g가 이 도메인 내의 다른 영역에 대해 다른 가중치를 적용하는 것을 원한다.<br><br>

---

(**중간정리** : **KPConv**는 격자 kernel이 아닌 **일정한 반지름 r을 가지는 구 형태의 kernel**을 사용한다. 2D Kernel의 grid 역할을 수행하는 것이 **구 형태의 kernel 내부의 kernel point**이다. 이 때 KNN보다 그냥 정해진 r값을 통한 구 형태의 kernel이 **밀도에 대해 강인**하고 네트워크가 의미 있는 표현을 학습하는 데 도움이 된다고 한다.)<br>
(출처 : https://shvblog.tistory.com/10)<br><br>

---

Kernel 속의 kernel point는 정해진 위치에서 각각의 weight를 가지고 존재한다.<br><br>

하지만 격자처럼 완벽히 인덱스가 맞는 것이 아니기에 input point feature와 이와 곱해질 kernel point weight를 결정하기 위해 위 식에 이어 아래의 식들을 통해 상관관계를 정의하고 새로운 feature를 생성한다.<br>
(출처 : https://shvblog.tistory.com/10)<br><br>

- Kernel point : <br>
$\{ \tilde{x}_{k} | k < K  \} \in \mathcal{B}_{r}^{3}$

<br>

- 특성을 차원 Din에서 Dout로 매핑하는 것에 관련된 가중치 행렬 : <br>
$\{ W_k | k < K \} \in \mathbb{R}^{D_{in} \times D_{out}}$

<br><br>

- **Kernel function** : $g(y_i)=\underset{k<K}{\sum} h(y_i,\tilde{x}_{k})W_k$<br>
<br>

$h$함수는 $\tilde{x}_{k}$와 $y_i$간의 상관관계이다. 이것은 $\tilde{x}_{k}$가 $y_i$에 가까울수록 커진다.<br>(Inspired by the bilinear interpolation)<br><br>

linear correlation은 다음과 같다.<br><br>
$ h(y_i,\tilde{x}_{k})=max(0, \ 1-\frac{\Vert y_i-\tilde{x}_{k} \Vert}{\sigma}) $<br><br>

$\sigma$는 kernel point의 영향 거리(influence distance)이다.<br>
이것은 input density에 따라 선정될 것이다.<br><br>

가우시안 상관 함수와 비교하면, 선형 상관 함수는 더 간단한 표현이다. 본 논문은 간단한 상관 함수를 권장하여 커널 변형을 학습할 때 그라디언트 역전파를 쉽게한다.<br>
(선형 함수를 사용하면 미분이 간단해진다. 예를 들어, 선형 상관 함수의 미분은 상수이며, 입력에 따라 변하지 않는다. 이는 역전파 과정에서 그라디언트를 효율적으로 전달할 수 있음을 의미한다)
<br><br>

<p align="center"><img src="/MyPDF/kpconv(1).png" width = "500" ></p>
<p align="center"> Figure 1. </p>

Figure 1을 다시 확인해보면 kernel point와 입력 point가 가까울 때 더 큰 feature값을 가지는 것을 설명하는 것을 확인할 수 있다.<br><br>
