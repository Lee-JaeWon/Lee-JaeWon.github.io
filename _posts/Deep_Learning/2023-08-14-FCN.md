---
title : "[논문 리뷰] Fully Convolutional Networks for Semantic Segmentation (FCN)"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---
[CVPR, 2015] Fully Convolutional Networks for Semantic Segmentation (FCN) 논문 리뷰

# Abstract
Convolutional network는 강력한 visual model로 hierarchies of features를 얻어낼 수 있다.

본 논문은 convolutional network 자체만으로도 end-to-end, pixels-to-pixels로 학습시켜, semantic segmentation에서 SOTA를 달성하는 것을 보인다.

본 논문의 핵심 아이디어는 임의 크기(arbitrary size)의 입력을 받아들이고 efficient한 inference와 learning을 통해 correspondingly-sized한 출력을 생성하는 **Fully convolutional** network를 구축하는 것이다.

fully convolutional network의 범위를 정의하고 설명하며, 이를 공간적으로 밀집된 예측 작업에 대한 적용을 설명하고 이전 모델과의 관련성을 도출한다.

또한 본 논문은 최신의 분류 네트워크(AlexNet, VGG, GoogLeNet)를 Fully convolutional network로 변경하고 segmentation 작업에 fine-tuning하여 학습된 표현을 전달한다.

이후, deep, coarse한 layer에서 얻은 **의미 정보**와 shallow, fine한 층에서 얻은 **외관 정보**를 결합하는 새로운 아키텍처를 정의하여 정확하고 자세한 segmentation을 생성한다.

(이후, SOTA를 이뤘다, 빠르다 에 관한 이야기)

# Introduction
Convolutional network는 recognition 분야를 이끌고 있다.

Convolutional network는 전체 이미지 분류뿐만 아니라 구조화된 출력을 가진 local task에서도 발전하고 있다.<br>
(bounding box object detection, part and key-point prediction, local correspondence등)

coarse에서 fine(미세한) 추론으로 진행하는 자연스러운 다음 단계는 이제 **모든 픽셀에서 예측을 수행**하는 것이다.

이전 연구에서는 semantic segmentation을 위해 convnets를 사용했으며, 각 픽셀은 해당 객체나 지역의 class로 레이블이 지정되었지만 이 작업에서 다루는 결점이 있다.

---

본 논문은 end-to-end, pixels-to-pixels로 학습되는 fully convolutional network(FCN)가 다른 기능 없이도 SOTA 결과를 냄을 보인다.

pixelwise prediction을 위해 FCN을 end-to-end로 학습하고 supervised pre-training으로 부터 학습하는 first work이다.

기존 네트워크의 fully convolutional version은 **임의 크기의 입력**에서 dense output을 예측한다.

learning과 inference는 dense feedforward computation과 back propagation에 의해 전체 이미지로 단번에 수행된다.(whole-image-at-a-time)

네트워크 내의 upsampling layer는 pixelwise prediction과 subsampled pooling된 네트워크 학습을 가능하게 한다.

이 방법은 절대적으로나 점근적으로 효율적이며 다른 작업에서의 복잡성을 제거한다.

Patchwise training은 흔하게 사용되지만 fully convolutional training의 효율성이 부족하다.

본 논문의 접근법은 전처리나 후처리를 하지 않는다.

본 모델은 분류에서의 최근 성공을 dense prediction으로 전환하여 classification net을 fully convolutional으로써 재해석하고 학습된 표현으로부터 fine-tuning한다.

---

Sematic segmentation은 semantic과 location에 대해 inherent tension에 직면한다.

global information은 what을 해결하고, local information은 where를 해결한다.

deep feature hierarchies는 **location과 semantic을** local-to-global pyramid로 **함께 인코딩**한다.

deep, coarse한 semantic information과 shallow, fine한 appearance information을 결합하기 위한 **새로운 "skip" architecture를 정의**한다.  

이 다음 깊은 분류 네트워크, FCN 및 합성곱을 사용한 최근 semantic segmentation 접근 방식에 대해 검토한다.

그 다음 FCN design과 dense prediction의 tradeoff를 설명하고, network 내 upsampling 및 multi-layer combination을 사용한 architecture를 소개하며, experimental framework를 설명한다.

이후, 여러 데이터셋에서 SOTA임을 보인다.

# 3. Fully convolutional networks

## 3.1 Adapting classifiers for dense prediction
전형적인 recognition 신경망은 AlexNet, LeNet, ... 및 더 깊은 후속 모델들을 포함하여 고정된 크기의 input을 받고 nonspatial한 output을 생성한다.

이러한 신경망의 fully connected layer는 **고정된 차원**을 가지고, 공간적인(spatial) **좌표를 무시**한다.

하지만 이러한 fully connected layer는 entire input region, 즉 입력 영역 전체를 커버하는 kernel로 볼 수도 있다.<br>
(전체 입력 값을 다루는 kernel을 가진 convolution으로 볼 수 있다.)

이를 통해, **어떠한 크기의 입력**도 받아들이고, classification map을 출력하는 full convolutional network로 변환할 수 있다.

<p align="center"><img src="/MyPDF/fcn(1).png" width = "500" ></p>
<p align="center">Figure 2</p>

이러한 변환은 위 Figure에 묘사되어있다.

게다가, resulting map은 기존 신경망을 특정 input patch에 대해 evaluation하는 것과 동등하다.

해당 패치들의 겹치는 영역을 기준으로 계산이 분할된다.<br>
(AlexNet은 227 X 227 이미지의 분류 점수를 생성하는 데 1.2 ms (일반적인 GPU에서))<br>
(fully convolutional version은 500 X 500 이미지에서 10 X 10 출력 그리드를 생성하는 데 22 ms가 소요된다. 이는 무작위 접근보다 5배 이상 빠르다.)

이러한 **convolutionalizaed**(컨볼루션화) model의 spatial output map은 segmantic segementation에 대한 자연스로운 선택지이다.

각 출력 셀마다 ground truth가 제공되어 forward 및 backward 전달이 모두 간단하며, 둘 다 convolution의 계산 효율성(and aggressive optimization)에서 장점을 얻는다.

AlexNet 예제에 해당하는 역전파에 대한 대응하는 시간은 단일 이미지에 대해 2.4ms이고,<br>
완전 합성곱 10 X 10 출력 맵에 대해서는 37ms으로,

순방향 전달과 유사한 속도 향상을 보인다.

<p align="center"><img src="/MyPDF/fcn(2).png" width = "500" ></p>
<p align="center">Figure 1</p>

본 논문이 classification net을 fully convolution으로 다시 해석함으로써 ouput map은 어떠한 임의의 크기의 입력에 대해서도 생성될 수 있지만,

출력 차원은 일반적으로 subsampling에 의해 감소된다.

classification net은 필터 크기를 작게 유지하고 computational requirements를 합리적으로 유지하기 위해 subsampling을 수행한다.

이로 인해 fully convoultuional version의 출력은 output unit의 receptive field의 pixel stride와 동일한 비율로 입력의 크기를 줄인다.

## 3.2 Shift-and-stitch is filter rarefaction
먼저 **결론은** Shift-and-stitch으로 초기 실험을 수행했지만 모델에는 결국사용하지 않았다.

Upsampling을 통한 학습이 더 효과적이고 효율적이며 추후에 skip layer fusion과 결합할 때 더 효과적이라 판단하였다고 한다.

---
(간단히 요약.. 사실 내용이 이해가 잘 안가는 파트이다.)<br>
Input shifting과 output interlacing는 OverFeat 논문에서 소개된 보간 없이 coarse한 출력에서 -> dense prediction을 생성하는 trick이다.

(다른 블로그에 잘 정리되어있다. [FCN 논문 리뷰](https://jlog1016.tistory.com/84))

Convnet의 filter와 layer stride만 변경하면 이러한 Shift-and-stitch 트릭과 동일한 출력을 생성할 수 있다.

<p align="center"><img src="/MyPDF/fcn(3).png" width = "500" ></p>

이 방법은 upsampling시, 기존 픽셀이 s로 나누어떨어지면 넓어진 영역의 픽셀로 들어가고 그렇지 않은 넓어진 영역들은 0으로 채워진다.

결론은 사용하지 않았다는 것.

## 3.3 Upsampling is backwards strided convolution
- backward strided convolution(=Deconvolution)
- Bilinear interpolation

Coarse한 출력을 dense한 픽셀로 바꾸는(연결하는) 또 다른 방법은 interpolation이다.

예를 들어, 간단한 bilinear interpolation은 각 출력 $y_{ij}$를 가장 가까운 네 개의 입력으로부터 상대적인 위치에만 의존하는 linear map을 통해 계산을 수행한다.

<p align="center"><img src="/MyPDF/fcn(6).png" width = "500" ></p>

---

어떤 의미에서, factor f로(f배로) upsampling하는 것은 input stride가 1/f인 convolution이다.<br>
(stride f를 통해 더 작은 이미지를 만드는 것의 반대)

f가 정수이면, 자연스러운 upsampling 방법은 output stride가 f인 backwards convolution(때로는 deconvolution이라고도 함)이다.

<p align="center"><img src="/MyPDF/fcn(4).png" width = "400" ></p>

구현하기 매우 간단하며, 그냥 컨볼루션의 forward와 backward passes를 반대로 수행하는 것이다.

따라서 upsampling은 pixelwise loss로부터 backpropagation을 통해 end-to-end 학습을 위해 네트워크 내에서 수행된다.

이러한 레이어의 deconvolution 필터는 고정되지 않아도 되며(예: 이중 선형 업샘플링), **학습될 수 있다.**

deconvolution 레이어의 stack과 activation function은 비선형 업샘플링을 학습할 수도 있다.

<p align="center"><img src="/MyPDF/fcn(5).png" width = "700" ></p>

## 3.4 Patchwise training is loss sampling
**결론은** 전체 이미지로 훈련하는 것이 효율적이고 효과적이다.

---

patchwise training과 fully-convolutional training 모두 어떤 분포든 생성할 수 있지만, 이들의 상대적인 계산 효율성은 겹침과 미니배치 크기에 따라 달라진다.

이미지에서 patch 이미지를 뽑아내서 필요한 부분을 위주로 training 시킬 수 있다.

patchwise training에서 샘플링은 class imbalance를 수정할 수 있으며, dense patch의 공간 상관 관계를 완화할 수 있다.

전체 이미지를 한 번에 사용하는 fully-convolutional training은, 각 배치가 모든 수용 영역을 포함하는 패치들로 구성되는 패치별 훈련과 동일한 결과를 얻을 수 있다.

실험 결과 전체 이미지로 훈련하는 것이 효율적이고 효과적이다.

# 4. Segmentation Architecture
ILSVRC 분류기를 FCN으로 변환하고, 네트워크 내에서 upsampling과 pixelwise loss을 사용하여 dense prediction을 위해 이를 보강한다.

segmentation을 위해 fine-tuning을 수행해 네트워크를 학습시킨다.

그 다음, coarse한 semantic과 local의 appearance 정보를 결합하여 refine prediction하는 새로운 **skip architecture**를 구축한다.

## 4.1 From classifier to dense FCN
본 논문에서는 ILSVRC12에서 우승한 AlexNet3 아키텍처과 ILSVRC14에서 탁월한 성과를 보인 VGG 넷 및 GoogLeNet을 고려한다.

그 중 VGG 16-layer를 선정하였다.

각 network를 마지막 **classifier layer를 버리고**, 모두 **fully convolution layer로 변환**한다.

coarse한 output 위치 각각에서 PASCAL 클래스 (배경 포함)의 점수를 예측하기 위해 채널 차원이 21인 1x1 컨볼루션을 추가하고,<br>
(channel을 PASCAL의 **class 개수와 맞추기 위해**)
<br>

이를 거친 뒤 [Section 3.3](#33-upsampling-is-backwards-strided-convolution)에서 설명한 대로 coarse한 output 위치의 예측을 pixel-dense 출력으로 bilinearly upsample하기 위한 deconvolution 레이어를 추가한다.

<p align="center"><img src="/MyPDF/fcn(7).png" width = "500" ></p>
<p align="center">Table 1</p>

이미 VGG로 already SOTA하다 라고 소개한다.

## 4.2 Combining what and where
<p align="center"><img src="/MyPDF/fcn(8).png" width = "800" ></p>
<p align="center">Figure 3</p>

<p align="center"><img src="/MyPDF/fcn(9).png" width = "500" ></p>
<p align="center">Figure 4</p>

최종 예측 레이어에서의 32 픽셀 스트라이드는 upsample된 출력의 **세부 사항의 스케일을 제한**한다.

이 문제를 해결하기 위해 final prediction layer와 더 낮은 스트라이드를 가진 lower layer를 **결합하는 링크**를 추가한다.

즉 깊은 레이어의 upsampling이 디테일에 한계를 지니고 있어,<br>
얕은 부분과 깊은 부분의 결합, coarse layer와 fine layer를 결합하면 전역 구조를 존중하며, 지역 예측을 수행할 수 있다.

이를 **Deep jet**이라 부른다.

- 1x1인 pool5의 activation map을 32배 한 upsampled prediction이 FCN-32s
- 2x2인 pool4의 activation map과 pool5를 2x upsampled prediction을 더하면 16x upsampled prediction(FCN-16s)
- 4x4 pool3과 앞서 더해주었던 prediction을 합하면 8x upsampled predicton(FCN-8s)

이러한 과정을 통해 개선을 얻었다고 한다.

<p align="center"><img src="/MyPDF/fcn(10).png" width = "500" ></p>
<p align="center">Table 2</p>

## 4.3 Experimental framework
[참고 : https://jlog1016.tistory.com/84](https://jlog1016.tistory.com/84)

- OptimizationSGD with momentummomentum : 0.9doubled the lr for biases
- dropout included in original classifer net
- weight decay : 5e-4, 2e-4
- lr : 10e-3, 10e−4 and 5e−5 for FCN-AlexNet, FCN-VGG16, and FCN-GoogLeNet

[Section 3.2](#32-shift-and-stitch-is-filter-rarefaction)에서 설명한 전체 이미지 사용 선택 및 학습에 관한 스펙을 설명한다.

<p align="center"><img src="/MyPDF/fcn(12).png" width = "500" ></p>
<p align="center">Figure 5</p>

# 5. Results
<p align="center"><img src="/MyPDF/fcn(11).png" width = "500" ></p>
<p align="center">Table 3</p>

Result 부분은 생략.

# 6. Conclusion
Fully convolutional networks는 다양한 모델 클래스 중 하나로, 현대의 classification convnet은 특별한 경우이다.

이를 인식하여 이러한 분류 네트워크를 segmentation으로 확장하고, 다중 해상도 레이어 조합을 통해 아키텍처를 개선하는 것은 현재 기술의 상태를 크게 개선시키는 동시에 학습과 추론을 단순화하고 가속화하였다.
