---
title : "[논문 리뷰] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---
[[ECCV, 2020] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934) 논문 리뷰

논문 내용만으로는 이해되지 않을 수 있어, 중간중간 **'중간 정리'**를 넣어놨습니다.

# Introduction
본 연구에서는 view synthesis(뷰 합성)의 오랜 문제에 새로운 방식으로 접근한다.

캡쳐된 이미지 집합의 rendering 오차를 최소화하기 위해 연속적인 5D 장면 표현의 매개변수를 직접 최적화하는 방식으로 접근한다.

정적인 장면을 공간의 각 점 $(x,y,z)$에서 각 방향 $(\theta, \phi)$으로 방출되는 광도(radiance)와 해당 점에서의 밀도(density)를 나타내며,<br>
각 point에서의 density는 ray가 $(x,y,z)$를 통과할 때 얼마나 많은 radiance가 축적되는지를 조절하는 differential opacity 역할을 한다.

본 논문의 방법은 convoultion layer가 없는 deep fully-connected neural network(often referred to as a MLP)을 최적화하여 이 함수를 표현하며,<br>
단일 좌표 $(x,y,z,\theta,\phi)$에서부터 single volume density와 view-dependent한 RGB color로 회귀한다.

<br>

특정 시점에서 **Neural Radiance field(NeRF)**를 렌더링하기 위해 다음과 같은 절차를 수행한다.

- 1: 카메라 광선을 장면으로 통과시켜 샘플링된 3D points 집합을 생성한다.
- 2: 이러한 점들과 이에 해당하는 2D 시야 방향(viewing direction)을 신경망의 입력으로 사용하여 출력 색상과 밀도의 집합을 생성한다.
- 3: 고전적인 volume 렌더링 기법을 사용하여 이러한 색상과 밀도를 2D 이미지로 누적한다.

<p align="center"><img src="/MyPDF/nerf(2).png" width = "700" ></p>
<p align="center">Figure 1</p>

<br>

이 과정은 **자연스럽게 미분이 가능**하므로, gradient descent를 사용하여 관측된 각 이미지와 본 논문의 representation에서 렌더링된 해당 뷰 간의 오류를 최소화하여 이 모델을 최소화할 수 있다.

다중 뷰에서 이 오류를 최소화하며 network는 실제 장면 내용을 포함하는 위치에서 volume density와 정확한 색상을 할당하여 장면의 일관된 모델을 예측하도록 한다.

Figure 2는 전체 파이프라인을 시각화한 것이다.

<p align="center"><img src="/MyPDF/nerf(1).png" width = "700" ></p>
<p align="center">Figure 2</p>

---

본 논문은 complex scene을 위한 nerual radiance field representation을 최적화하는 기본 구현이 충분히 고해상도로 수렴하지 않으며, camera ray당 필요한 필요한 샘플 수가 **비효율적임을 발견**했다.

이러한 문제들을 다루기 위해 입력 5D 좌표를 **positional encoding**으로 변환하여 MLP가 더 높은 주파수 함수를 표현할 수 있게 했고,<br>
높은 주파수 scene representation을 충분히 샘플링하고, 필요한 query수를 줄이는 **hierarchical sampling procedure(계층적 샘플링 절차)**를 제안했다.

---

NeRF는 volumetric representation의 이점을 상속(계승)받았다.

둘 다 복잡한 현실 세계의 기하학적인 형태와 외관을 나타낼 수 있으며,
투영된 이미지를 사용한 gradient-based 최적화에 적합하다.

중요한 점은 본 논문의 방법이 고해상도에서 복잡한 장면을 모델링할 때 이산화된 voxel-grid의 저장 비용을 극복한다.

NeRF의 기술적 기여는 다음과 같다.

- MLP 네트워크로 parameterized된 5D neural radiance field를 사용하여 복잡한 기하학적 형태와 material을 포함하는 연속적인 장면을 나타내는 방식을 제안
- classical volume rendering technique을 기반으로 한 미분 가능한 렌더링 방식, 이를 통해 표준 RGB이미지에서 이러한 표현을 최적화한다.
- 각 입력 5D 좌표를 더 높은 차원의 공간으로 매핑하는 positional encoding, 이를 통해 고주파 장면 내용을 나타내기 위해 neural radiance field를 성공적으로 최적화할 수 있다.

<br>

# Neural Radiance Field Scene Representation
본 논문은 연속적인 장면을 5D vector-valued function으로 나타내며,<br>
**입력**은 3D 위치 $(x,y,z)$와 2D viewing direction $(\theta, \phi)$이다.

**출력**은 방출된 색상 $C=(r,g,b)$와 volume density $\sigma$이다.

실제로는 방향을 3D cartesian unit vector $d$로 표현한다.

본 논문은 MLP network $F_{\Theta}:(x,d) \to (c,\sigma)$를 사용하여 **입력 5D 좌표마다** 해당 부피 밀도와 directional emitted color를 매핑하도록 가중치 $\Theta$를 최적화하여 연속적인 5D 장면 표현을 추정한다.

본 논문은 이러한 표현이 multiview에 일관성을 갖도록 하기 위해, network가 volume density $\sigma$를 **위치 $x$의 함수로만 예측**하도록 제한하였고,

RGB 색상 $c$는 위치와 viewing direction **둘의 함수로 예측**할 수 있도록 했다.

이를 위해 MLP $F_{\Theta}$는 입력 3D 좌표 $x$를 7개의 fully-connected layer를 사용하여 처리하고 $\sigma$와 256차원의 feature vector를 출력한다.<br>
(ReLU 활성화 함수와 layer당 256 채널)

이 feature vector는 camera ray의 viewing direction과 연결되어 하나의 추가적인 fully-connected layer를 통과하며 view-dependent한 RGB 색상을 출력한다.<br>
(ReLU 활성화 함수와 128 채널)

Non-Lambertian 효과를 나타내기 위해 input viewing direction을 사용하는 방법은 아래 Figure 3과 같다.<br>
(Non-Lambertian 효과 : 보는 각도에 따라 색이나 반사율이 다르다)

<p align="center"><img src="/MyPDF/nerf(3).png" width = "700" ></p>
<p align="center">Figure 3</p>

Figure 4에는 view dependence가 없는 모델은 specularity(반사성? 광택?)을 표현하는데 어려움을 겪는다.

<p align="center"><img src="/MyPDF/nerf(4).png" width = "700" ></p>
<p align="center">Figure 4</p>

🦾**중간 정리**<br>
원하는 것 : N개의 시점에서 찍은 2D image를 활용해 -> **임의의 시점**에서의 2D Image를 얻을 수 있는가?<br><br>
단순히 3D 공간상의 **어떠한 점을 입력**으로 넣고 -> 그 부분의 RGB값과 density를 구하고 -> 렌더링 하겠다.<br>(이 과정에서 MLP를 활용)<br><br>
[Figure 2](../../MyPDF/nerf(1).png)가 잘 표현한 것 같다.
{: .notice--info}

<br>

# Volume Rendering with Radiance Fields
본 논문의 5D neural radiance field는 장면을 volume density와 공간의 어떤 지점에서의 directional emitted radiance로 나타낸다.

classical volume rendering의 원리를 사용하여 장면을 통과하는 모든 ray의 색상을 렌더링한다.

**volume density $\sigma(x)$는 위치 $x$에 존재하는 극소한 입자에서 사라지는 ray가 미분이 가능할 확률로 해석할 수 있다.**

camera ray $r(t)=o+td$의 예상 색상 $C(r)$은 근처와 먼 경계 $t_n$과 $t_f$와 함께 다음 식과 같다.

<p align="center"><img src="/MyPDF/nerf(5).png" width = "700" ></p>

함수 $T(t)$는 ray가 $t_n$부터 $t$까지 객체를 통과하며, 축적된 투과율(accumulated transmittance)이다.

즉, 광선이 다른 입자에 **부딪히지 않고** $t_n$에서 $t$까지 이동하는 확률을 의미한다.<br>
(우리가 바라본 시점에 객체가 없으면 멀리 있는 것도 보이지만 앞에 불투명한 객체가 있으면 가까이 있는 객체만 보이기에 적절한 표현, [출처](https://velog.io/@minkyu4506/NeRF-Representing-Scenes-asNeural-Radiance-Fields-for-View-Synthesis-%EB%A6%AC%EB%B7%B0))

연속적인 neural radiance field에서의 view rendering은 // 원하는 virtual camera의 **각 픽셀을 통해 추적된** camera ray에 대해 // $C(r)$이라는 적분을 추정하는 것을 필요로 한다.

🦾**중간 정리**<br>
결국 2D image에서 보이는 것은 어떠한 픽셀에서의 RGB값이다.<br><br>
하지만 우리는 Radiance field, 즉 공간상의 ray가 어떠한 RGB값과 density값을 가지는지를 위에서 MLP를 통해 얻었었다.<br><br>
이 점들을 RGB값과 density를 이용해 적분하여 2D image에서 어떠한 예상 색상을 가지는지를 구하는 것이 요점이다.<br><br>
이 때, 공기같은 것은 density가 낮다(투과율이 높다). 이러한 애들을 적분하여 실제 물체가 있는 곳(denstiy가 높은 곳)에서의 색상이 더 많은 weight를 갖도록 하는 weighted sum을 수행한다고 생각하면 될 것 같다.
{: .notice--info}

본 논문은 이 continuous integral을 수치적으로 quadrature를 사용하여 추정한다.

discretized voxel grid 렌더링을 위해 보통 사용되는 **Deterministic quadrature**는 MLP가 오직 고정된 discrete 위치 집합(fixed discrete set of location)에서만 query되므로 효과적으로 representation의 해상도를 **제한할 수 있다.**<br>
(적분할 때 쓰는 값의 개수가 항상 같기 때문에 가상의 레이저가 접촉하는 좌표가 객체가 아닌 공기일 수도 있다는 것, [출처](https://velog.io/@minkyu4506/NeRF-Representing-Scenes-asNeural-Radiance-Fields-for-View-Synthesis-%EB%A6%AC%EB%B7%B0))<br>
(이게 안좋다는 것을 이야기하는 것 같은데 왜 effectively하다고 하는지 이해가 가지 않는다.)

대신, 본 논문은 $[tn, tf]$를 N개의 일정 간격으로 분할한 다음 각 구간 내에서 균일하게 무작위로 하나의 샘플을 추출하는 **stratified sampling** 접근 방식을 사용한다.

<p align="center"><img src="/MyPDF/nerf(6).png" width = "700" ></p>
<p align="center">Eqn (2)</p>

적분을 추정하기 위해 discrete set of samples를 사용하긴 하지만, stratified sampling을 통해 최적화 과정에서 **MLP가 연속적인 위치에서 평가되도록 하여** 연속적인 장면을 나타낼 수 있다.

이러한 샘플을 이용하여 $C(r)$을 추정한다.

<p align="center"><img src="/MyPDF/nerf(7).png" width = "700" ></p>
<p align="center">Eqn (3)</p>

여기서 $\delta_{i}=t_{i+1}-t_{i}$은 인접한 샘플간 거리이다.

$(c_i, \sigma_{i})$ 값 집합으로부터 $\hat{C}(r)$을 계산하는 이 함수는 간단하게 미분이 가능하며,<br>
$\alpha_{i} = 1-exp(-\sigma_{i}\delta_{i})$와 traditional alpha compositing으로 축소된다.


🦾**중간 정리**<br>
실제로는 continous한 적분을 수행할 수 없기에 discrete한 형태로 변환하여 위 같은 식을 사용했다.
{: .notice--info}

<br>

# Optimizing a Neural Radiance Field
이전 섹션에서 neural radiance field로 scene을 모델링하고 이 표현에서 새로운 view를 렌더링하기 위해 필요한 핵심 구성 요소를 설명했다.

하지만 본 논문에서 이러한 구성만으로는 SOTA를 달성하는데 충분하지 않음을 관찰했다.

**두 가지 개선 사항**을 소개하여, 고해상도의 complex scene들을 효과적으로 표현하고자 한다.

첫 번째는 입력 좌표의 **positional encoding**이다.<br>
이는 MLP의 High-frequency function을 표현하는데 도움을 주며,

두 번째는 이 High-frequency 표현을 효율적으로 샘플링할 수 있는 **hierarchical sampling procedure**이다.

## Positional encoding
비록 nerual network가 범용적인 함수의 근사로 알려져있지만,<br>
본 논문에서 $F_{\Theta}$가 **직접** $xyz\theta \phi$ 입력 좌표에서 작동하는 것이 색상과 geometry에서 **high-frequency 변동을 잘 표현하지 못하고** 렌더링 결과가 좋지 않음을 발견했다.

또 다른 연구(Rahaman 등의 연구)에서 깊은 신경망이 **낮은 주파수 함수를 학습하는 데 편향**되어 있다는 것을 보여으며,

그들은 또한 **입력을 더 높은 차원의 공간으로 매핑**하고 낮은 주파수 함수가 아닌 **고주파수 함수를 사용**하여 네트워크로 전달하는 것이 고주파수 변동을 포함하는 데이터에 더 들어맞도록 하는 것을 가능하게 한다는 것을 보였다고 한다.

본 논문에서 nerual scene representation의 맥락에서 이러한 발견을 활용하고,<br>
$F_{\Theta}$를 두 함수의 조합인 $F_{\Theta}=F_{\Theta}' \circ \gamma$로 재구성함으로써 성능을 크게 향상시킬 수 있음을 보인다.

여기서 $\gamma$는 $\mathbb{R}$에서 더 높은 차원의 공간인 $\mathbb{R}^{2L}$으로의 매핑이며,<br>
$F_{\Theta}'$는 여전히 일반적인 MLP이다.

사용하는 인코딩 함수는 다음과 같다.

<p align="center"><img src="/MyPDF/nerf(9).png" width = "700" ></p>

이 함수 $\gamma(\cdot)$는 $x$의 세 개의 좌표 값 각각에 따로 적용된다.<br>
($x$값들은 [-1,1]범위로 정규화되있다.)

그리고 cartesian viewing direction unit vector인 $d$의 세 가지 성분에도 동일하게 적용한다.<br>
(이 벡터는 구조적으로 [-1,1]에 존재한다.)

실험에서는 $\gamma(x)$에 대해 L = 10으로 설정하였고, $\gamma(d)$에 대해 L = 4로 설정하였다.

<p align="center"><img src="/MyPDF/nerf(8).png" width = "700" ></p>

🦾**중간 정리**<br>
이러한 함수들을 사용하여 연속적인 입력 좌표를 더 높은 차원 공간으로 매핑하여 MLP가 더 쉽게 더 높은 주파수 함수를 근사할 수 있도록 한다.
{: .notice--info}

## Hierarchical volume sampling
본 논문의 각 camera ray별로 N개의 query point에서 neural radiance field netowrk를 밀집하게 평가하는 렌더링 전략은 비효율적이다.<br>
(camera ray에서 N개의 포인트를 임의로 뽑은 후 렌더링에 사용하는 것이 비효율적)

왜냐하면 **렌더링된 이미지에 기여하지 않는** free space 및 가려진 영역도 여전히 반복적으로 샘플링되기 때문이다.

그래서 본 논문에서는 마지막 렌더링에서 예측되는 효과에 비례하여 샘플을 할당하여 렌더링 효율성을 높이는 hierarchical representation을 제안한다.

장면을 나타내기 위해 단순히 single network를 사용하기보다,<br>
두 개의 network를 동시에 최적화한다.

하나는 Coarse 네트워크이고, 다른 하나는 fine 네트워크이다.

먼저 stratified sampling을 사용하여 $N_c$개의 위치 집합을 샘플링하고,<br>
[Eqn (2)](../../MyPDF/nerf(6).png)와 [Eqn (3)](../../MyPDF/nerf(7).png)을 이용하여 이러한 위치에서 coarse 네트워크를 평가한다.

이 coarse한 네트워크의 출력을 고려할 때, volume의 관련 부분으로 편향된 각각의 ray를 따라 more informed된 샘플링을 생성한다.

이를 위해 [Eqn (3)](../../MyPDF/nerf(7).png)의 Coarse 네트워크 $\hat{C}_c(r)$에서 얻은 alpha composited color를<br>
모든 sampled color의 weighted sum으로 다시 작성한다.

<p align="center"><img src="/MyPDF/nerf(10).png" width = "700" ></p>
<p align="center">Eqn 5</p>

이러한 가중치를 [$\hat{w}_i=w_i / \sum w_j $](../../MyPDF/nerf(11).png)으로 정규화하면, ray별로 piecewise-constant PDF(확률밀도함수)가 생성된다.

<p align="center"><img src="/MyPDF/nerf(11).png" width = "300" ></p>

---

그 다음 inverse transform sampling을 사용하여 현재 본포에서 두번째 set인 $N_f$ 위치의 집합을 샘플링한다.

이후 첫 번째와 두 번째 샘플 집합의 합집합에서 fine network를 평가하고,<br>
모든 $N_c+N_f$ 샘플을 사용하여 ray의 최종 렌더링된 색상 $\hat{C}_f(r)$을 [Eqn (3)](../../MyPDF/nerf(7).png)을 이용하여 계산한다.

🦾**중간 정리**<br>
먼저 $N_c$를 활용하여 전체 ray에서 sampling된 점들을 사용하는 coarse 네트워크를 학습한다.<br><br>
그러면 [아래 그림](../../MyPDF/nerf(13).png)처럼 물체가 있어보이는 구간에 대해 다시 두 번째 set $N_f$를 생성하는 것 같다.<br><br>
이제 이 둘을 합친 샘플을 사용하여 ray의 최종 색상 C를 계산한다.
{: .notice--info}

<p align="center"><img src="/MyPDF/nerf(13).png" width = "500" ></p>

# Implementation details
본 논문은 각 scene에 대해 분리된 neural continuous volume representation 네트워크를 최적화한다.

이것은 scene의 캡쳐된 RGB 이미지 데이터셋과 해당하는 카메라 포즈와 내부 파라미터, 그리고 장면 경계가 필요하다.<br>
(we use ground truth camera poses, intrinsics, and bounds for synthetic data)

그리고 COLMAP이라는 sfm 패키지를 사용하여 실제 데이터에 대해 이러한 매개 변수를 추정한다.

각 최적화 반복에서, 데이터셋의 모든 픽셀에서 카메라 광선의 배치를 무작위로 샘플링한 다음,<br>
[Hierarchical volume sampling](#hierarchical-volume-sampling)에서 설명한 계층적 샘플링을 이용해 Coarse 네트워크의 $N_c$ 샘플과 fine 네트워크의 $N_c+N_f$ 샘플을 query한다.

다음, [Volume Rendering with Radiance Fields](#volume-rendering-with-radiance-fields)에서 설명한 volume rendering procedure를 사용하여 두 개의 샘플 집합에서 각 ray의 색상을 렌더링한다.

본 논문의 loss는 간단하게 coarse와 fine 모두에서 rendered color와 true pixel의 color간의 total squared error이다.

<p align="center"><img src="/MyPDF/nerf(12).png" width = "700" ></p>
<p align="center">Eqn 6</p>

R은 각 batch에 있는 camera ray의 집합이다.

# 참고자료
[[논문리뷰]NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis 리뷰](https://velog.io/@minkyu4506/NeRF-Representing-Scenes-asNeural-Radiance-Fields-for-View-Synthesis-%EB%A6%AC%EB%B7%B0)

아래 영상은 포스팅이후 생각보다 이해가 안가서 공부하려고 본 영상인데,<br>
가장 명확하게 설명되어있었습니다.<br>
[Hyeongmin Lee님의 NeRF 리뷰 영상](https://www.youtube.com/watch?v=zkeh7Tt9tYQ)

📣<br>
포스팅에서 오류나 궁금한 점은 Comments를 작성해주시면, 많은 도움이 됩니다.💡
{: .notice--info}