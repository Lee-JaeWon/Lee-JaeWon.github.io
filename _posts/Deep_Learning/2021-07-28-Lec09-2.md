---
title : "Lec 09-2: Weight Initialization[Include Pytorch Code]"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Deep_Learning study Lec09-2  

## Boostcourseì˜ 'íŒŒì´í† ì¹˜ë¡œ ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ì´ˆ'ë¥¼ í†µí•œ ê³µë¶€ì™€ ì •ë¦¬ Post    

## Goal of Study  
> - ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Inititalization)ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.  

### Keyword
> - ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”(Weight Inititalization)   
> - Xavier / He inititalization    

## 1. ê°•ì˜ ìš”ì•½  
### RBM(Restricted Boltzmann Machine)

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127283813-a5e3b64e-ff74-4762-ae7e-4a3b3f71b794.png" width = "400" ></p>  

Restricted : no connections within a layer  
ê°™ì€ layerì•ˆì— ìˆëŠ” ë…¸ë“œë“¤ ë¼ë¦¬ëŠ” ì—°ê²°ì´ ì—†ë‹¤. ê°™ì€ ë ˆì´ì–´ì˜ ë…¸ë“œë¼ë¦¬ ì—°ê²°ì´ **ì œí•œ**ë˜ì–´ ìˆì–´ ëª¨ë¸ì˜ ì´ë¦„ì— Restrictedê°€ ë¶™ì€ ê²ƒì´ë‹¤.

í•˜ì§€ë§Œ, ë‹¤ë¥¸ layer ì‚¬ì´ì—ì„œëŠ” ê° ë…¸ë“œë“¤ì´ ëª¨ë‘ ì—°ê²°ë˜ì–´ ìˆëŠ” í˜•íƒœ(fully connected)ë¥¼ RBMì´ë¼ê³  í•œë‹¤.  

ìš”ì¦˜ì€ ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  ì•Œë ¤ì ¸ìˆë‹¤.

### Xavier Initialization
2010ë…„ ì„¸ì´ë¹„ì–´ ê¸€ë¡œëŸ¿ê³¼ ìš”ìŠˆì•„ ë²¤ì§€ì˜¤ê°€ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ê°€ ëª¨ë¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì—¬ ìƒˆë¡œìš´ ì´ˆê¸°í™” ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤.

ì´ ì´ˆê¸°í™” ë°©ë²•ì€ ì œì•ˆí•œ ì‚¬ëŒì˜ ì´ë¦„ì„ ë”°ì„œ ì„¸ì´ë¹„ì–´(Xavier Initialization) ì´ˆê¸°í™” ë˜ëŠ” ê¸€ë¡œëŸ¿ ì´ˆê¸°í™”(Glorot Initialization)ë¼ê³  í•œë‹¤.

Xavierì™€ ë’¤ì—ì„œ ë‹¤ë£° He Initializationì€ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Layerì˜ íŠ¹ì„±ì— ë”°ë¼ì„œ ë‹¤ë¥´ê²Œ Initializationì„ í•´ì•¼ ëœë‹¤ëŠ” ë‚´ìš©ì´ë‹¤.

#### Xavier Normal Initialization
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127287798-f15d5edb-29dc-4a8d-b4f4-0f4e55738f28.png" width = "300" ></p>

ìœ„ ì‹ì—ì„œ í‰ê· ì€ 0, í‘œì¤€í¸ì°¨(Standard deviation)ëŠ” $ Var(W) $ìˆ˜ì‹ì„ ì´ìš©í•´ ì´ˆê¸°í™”ë¥¼ í•˜ë©´ ëœë‹¤ëŠ” ë‚´ìš©ì´ë‹¤.

#### Xavier Uniform Initialization
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127287875-ccf2fa7b-5d9b-498c-a4d6-61c6c5f462cf.png" width = "300" ></p>

ì´ ë˜í•œ, ìœ„ì˜ ìˆ˜ì‹ì„ ì´ìš©í•´ $W$ë¥¼ ì´ˆê¸°í™”í•˜ë©´ ì ì ˆíˆ ì´ˆê¸°í™”ê°€ ëœë‹¤ëŠ” ë‚´ìš©ì´ë‹¤. 

ì—¬ê¸°ì„œ, $ n_{in} $ì€ Layerì˜ inputì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ê³ , $ n_{out} $ì€ outputì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

ê²°ë¡ , RBMìœ¼ë¡œ layerë§ˆë‹¤ í•™ìŠµí•˜ê³  Fine-tuningí•˜ëŠ” ë³µì¡í•œ ë‹¨ê³„ê°€ í•„ìš”ì—†ì´ ê°„ë‹¨í•œ ìˆ˜ì‹ë§Œìœ¼ë¡œ Weight Initializationì´ ê°€ëŠ¥í•˜ë‹¤.

### He Initialization
ì´ëŠ”, Xavier Initializationì˜ ë³€í˜•ì´ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë©°,  

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127289031-12ecd01a-cd1e-4e68-be6e-35f76d9424b8.png" width = "300" ></p>

ë§ˆì°¬ê°€ì§€ë¡œ, Normalê³¼ Uniformì˜ í˜•íƒœê°€ ìˆë‹¤.

ìˆ˜ì‹ì˜ í˜•íƒœë„ ìƒë‹¹íˆ ë¹„ìŠ·í•œë°, Xavier Initializationì—ì„œ $ n_{out} $ termì´ ì—†ì–´ì§„ í˜•íƒœë¼ ë³¼ ìˆ˜ ìˆë‹¤.

### Result With Xavier Initialization
(ê°œë…ì— ê´€í•œ í¬ìŠ¤íŒ…ì€ ëë‚¬ìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” í•™ìŠµì‹œì¼œë³¸ ì½”ë“œë¥¼ ì˜¬ë ¤ë†“ì•˜ìŠµë‹ˆë‹¤.)  

#### mnist_nn_xavier
```python
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import random
```


```python
USE_CUDA = torch.cuda.is_available() # GPUë¥¼ ì‚¬ìš©ê°€ëŠ¥í•˜ë©´ True, ì•„ë‹ˆë¼ë©´ Falseë¥¼ ë¦¬í„´
device = torch.device("cuda" if USE_CUDA else "cpu") # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©í•˜ê³  ì•„ë‹ˆë©´ CPU ì‚¬ìš©
print("ë‹¤ìŒ ê¸°ê¸°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤:", device)

# for reproducibility
random.seed(777)
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
```

    ë‹¤ìŒ ê¸°ê¸°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤: cpu
    


```python
# parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100

# MNIST dataset
mnist_train = dsets.MNIST(root='E:\DeepLearningStudy',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

mnist_test = dsets.MNIST(root='E:\DeepLearningStudy',
                         train=False,
                         transform=transforms.ToTensor(),
                         download=True)

# dataset loader
data_loader = torch.utils.data.DataLoader(dataset=mnist_train,
                                          batch_size=batch_size,
                                          shuffle=True,
                                          drop_last=True)
```

    C:\Users\LeeJaeWon\anaconda3\envs\DeepLearningStudy\lib\site-packages\torchvision\datasets\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\torch\csrc\utils\tensor_numpy.cpp:180.)
      return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
    


```python
# nn layers
linear1 = torch.nn.Linear(784, 256, bias=True)
linear2 = torch.nn.Linear(256, 256, bias=True)
linear3 = torch.nn.Linear(256, 10, bias=True)
relu = torch.nn.ReLU()
```


```python
# xavier initialization
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)
```




    Parameter containing:
    tensor([[-0.0215, -0.0894,  0.0598,  ...,  0.0200,  0.0203,  0.1212],
            [ 0.0078,  0.1378,  0.0920,  ...,  0.0975,  0.1458, -0.0302],
            [ 0.1270, -0.1296,  0.1049,  ...,  0.0124,  0.1173, -0.0901],
            ...,
            [ 0.0661, -0.1025,  0.1437,  ...,  0.0784,  0.0977, -0.0396],
            [ 0.0430, -0.1274, -0.0134,  ..., -0.0582,  0.1201,  0.1479],
            [-0.1433,  0.0200, -0.0568,  ...,  0.0787,  0.0428, -0.0036]],
           requires_grad=True)




```python
# model
model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)
```


```python
# define cost/loss & optimizer
criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```


```python
for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = len(data_loader)

    for X, Y in data_loader:
        # reshape input image into [batch_size by 784]
        # label is not one-hot encoded
        X = X.view(-1, 28 * 28).to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))

print('Learning finished')
```

    Epoch: 0001 cost = 0.246781081
    Epoch: 0002 cost = 0.092835627
    Epoch: 0003 cost = 0.062099934
    Epoch: 0004 cost = 0.043158282
    Epoch: 0005 cost = 0.032552719
    Epoch: 0006 cost = 0.024990607
    Epoch: 0007 cost = 0.021460216
    Epoch: 0008 cost = 0.019880820
    Epoch: 0009 cost = 0.015151084
    Epoch: 0010 cost = 0.015341577
    Epoch: 0011 cost = 0.012235122
    Epoch: 0012 cost = 0.012357231
    Epoch: 0013 cost = 0.010972363
    Epoch: 0014 cost = 0.008886398
    Epoch: 0015 cost = 0.008259354
    Learning finished
    


```python
# Test the model using test sets
with torch.no_grad():
    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = model(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy:', accuracy.item())

    # Get one and predict
    r = random.randint(0, len(mnist_test) - 1)
    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)
    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)

    print('Label: ', Y_single_data.item())
    single_prediction = model(X_single_data)
    print('Prediction: ', torch.argmax(single_prediction, 1).item())
    
    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')
    plt.show()
```

    Accuracy: 0.9807999730110168
    Label:  9
    Prediction:  9
    

    C:\Users\LeeJaeWon\anaconda3\envs\DeepLearningStudy\lib\site-packages\torchvision\datasets\mnist.py:67: UserWarning: test_data has been renamed data
      warnings.warn("test_data has been renamed data")
    C:\Users\LeeJaeWon\anaconda3\envs\DeepLearningStudy\lib\site-packages\torchvision\datasets\mnist.py:57: UserWarning: test_labels has been renamed targets
      warnings.warn("test_labels has been renamed targets")
    
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127291787-8f967e2f-f87a-440f-a1f0-3cdc7ee76abd.png" width = "400" ></p>
    
ì´ì „ì— ê¸°ë³¸ MNISTí•™ìŠµí•  ë•Œ `Normal_` distributionìœ¼ë¡œ ì´ˆê¸°í™” í•  ë•Œë³´ë‹¤ ì²´ê°ìƒ, ìˆ˜ì¹˜ìƒ Accuracyê°€ ë” ë†’ì€ ê²ƒ ê°™ë‹¤.

#### mnist_nn_deep
ì¡°ê¸ˆ ë” deepí•˜ê²Œ í•™ìŠµí•´ë³´ì.

```python
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import random
```


```python
USE_CUDA = torch.cuda.is_available() # GPUë¥¼ ì‚¬ìš©ê°€ëŠ¥í•˜ë©´ True, ì•„ë‹ˆë¼ë©´ Falseë¥¼ ë¦¬í„´
device = torch.device("cuda" if USE_CUDA else "cpu") # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©í•˜ê³  ì•„ë‹ˆë©´ CPU ì‚¬ìš©
print("ë‹¤ìŒ ê¸°ê¸°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤:", device)

# for reproducibility
random.seed(777)
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
```

    ë‹¤ìŒ ê¸°ê¸°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤: cpu
    


```python
# parameters
learning_rate = 0.001
training_epochs = 15
batch_size = 100

# MNIST dataset
mnist_train = dsets.MNIST(root='E:\DeepLearningStudy',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

mnist_test = dsets.MNIST(root='E:\DeepLearningStudy',
                         train=False,
                         transform=transforms.ToTensor(),
                         download=True)

# dataset loader
data_loader = torch.utils.data.DataLoader(dataset=mnist_train,
                                          batch_size=batch_size,
                                          shuffle=True,
                                          drop_last=True)
```


```python
# nn layers
linear1 = torch.nn.Linear(784, 512, bias=True)
linear2 = torch.nn.Linear(512, 512, bias=True)
linear3 = torch.nn.Linear(512, 512, bias=True)
linear4 = torch.nn.Linear(512, 512, bias=True)
linear5 = torch.nn.Linear(512, 10, bias=True)
relu = torch.nn.ReLU()
```


```python
# xavier initialization
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)
torch.nn.init.xavier_uniform_(linear4.weight)
torch.nn.init.xavier_uniform_(linear5.weight)
```




    Parameter containing:
    tensor([[-0.0565,  0.0423, -0.0155,  ...,  0.1012,  0.0459, -0.0191],
            [ 0.0772,  0.0452, -0.0638,  ...,  0.0476, -0.0638,  0.0528],
            [ 0.0311, -0.1023, -0.0701,  ...,  0.0412, -0.1004,  0.0738],
            ...,
            [ 0.0334,  0.0187, -0.1021,  ...,  0.0280, -0.0583, -0.1018],
            [-0.0506, -0.0939, -0.0467,  ..., -0.0554, -0.0325,  0.0640],
            [-0.0183, -0.0123,  0.1025,  ..., -0.0214,  0.0220, -0.0741]],
           requires_grad=True)




```python
# model
model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)
```


```python
# define cost/loss & optimizer
criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```


```python
for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = len(data_loader)

    for X, Y in data_loader:
        # reshape input image into [batch_size by 784]
        # label is not one-hot encoded
        X = X.view(-1, 28 * 28).to(device)
        Y = Y.to(device)

        optimizer.zero_grad()
        hypothesis = model(X)
        cost = criterion(hypothesis, Y)
        cost.backward()
        optimizer.step()

        avg_cost += cost / total_batch

    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))

print('Learning finished')
```

    Epoch: 0001 cost = 0.285531998
    Epoch: 0002 cost = 0.090530835
    Epoch: 0003 cost = 0.058814172
    Epoch: 0004 cost = 0.039957844
    Epoch: 0005 cost = 0.031853903
    Epoch: 0006 cost = 0.025514400
    Epoch: 0007 cost = 0.020236425
    Epoch: 0008 cost = 0.019569544
    Epoch: 0009 cost = 0.014461165
    Epoch: 0010 cost = 0.014499786
    Epoch: 0011 cost = 0.014732383
    Epoch: 0012 cost = 0.011545545
    Epoch: 0013 cost = 0.012704160
    Epoch: 0014 cost = 0.010244285
    Epoch: 0015 cost = 0.008064025
    Learning finished
    


```python
# Test the model using test sets
with torch.no_grad():
    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = model(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy:', accuracy.item())

    # Get one and predict
    r = random.randint(0, len(mnist_test) - 1)
    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)
    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)

    print('Label: ', Y_single_data.item())
    single_prediction = model(X_single_data)
    print('Prediction: ', torch.argmax(single_prediction, 1).item())
    
    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')
    plt.show()
```

    Accuracy: 0.9767000079154968
    Label:  8
    Prediction:  8

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127294669-5b998899-1ad7-4d42-9f46-b4bd9b2e7897.png" width = "400" ></p>

ë”°ë¡œ ì½”ë“œë¥¼ [MyGitHub_MNIST_Xavier,Deep](https://github.com/Lee-JaeWon/DeepLearning_Study_tutorial/blob/main/DeepLearning_with_pytorch/pytorch_Lab-09-Xavier%2Cdeep.ipynb)ì— ì˜¬ë ¤ë†“ì•˜ë‹¤.

ğŸ“£<br>
ë³¸ í¬ìŠ¤íŒ…ì˜ í•™ìŠµ í™˜ê²½ : `Anaconda`, `CPU`, `Pytorch`, `Jupyter Notebook`  
í¬ìŠ¤íŒ…ì—ì„œ ì˜¤ë¥˜ë‚˜ ê¶ê¸ˆí•œ ì ì€ Commentsë¥¼ ì‘ì„±í•´ì£¼ì‹œë©´, ë§ì€ ë„ì›€ì´ ë©ë‹ˆë‹¤.ğŸ’¡
{: .notice--info}

## ì°¸ê³ ìë£Œ
[Pytorchë¡œ ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ ì…ë¬¸](https://wikidocs.net/61271)  
[Deep Learning Zero To All _ GitHub](https://github.com/deeplearningzerotoall/PyTorch)