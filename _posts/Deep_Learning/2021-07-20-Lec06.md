---
title : "Deep_Learning study Lec06"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Lec 06: Softmax Regression  

## Boostcourse의 '파이토치, 텐서플로우로 시작하는 딥러닝 기초'를 통한 공부와 정리 Post    

## Goal of Study  
> - 다항 분류(Multinomial Classification)의 개념을 알아본다. 

### Keyword
> - 다항 분류(Multinomial Classification)   
> - 소프트맥스(Softmax)  
> - 크로스 엔트로피(Cross Entropy)  

## 1. 강의 요약  
### One-Hot Enconding
컴퓨터나 기계적 프로세스들은 문자보다 숫자를 더 잘 처리할 수 있다.  

그래서 문자를 숫자로 바꾸는 여러가지 기법 중 가장 기본적인 표현 방법이 One-Hot Enconding이다.

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다.  

이렇게 표현된 벡터를 **원-핫 벡터(One-Hot vector)**라고 한다.  
  
예를 들어보자면,  
강아지, 고양이, 냉장고라는 3개의 선택지가 있을 때  

강아지 = [1, 0, 0]  -> 0번 index  
고양이 = [0, 1, 0]  -> 1번 index  
냉장고 = [0, 0, 1]  -> 2번 index  

다음과 같이 정수로 부여된 인덱스를 원-핫 인코딩을 하게 되면 위와 같은 벡터를 가지게 된다.  

꼭, One-Hot vector로 표현해야 다중 클래스 분류에 관한 문제를 풀 수 있는 것은 아니지만,  
다중 클래스 분류 문제가 각 **클래스 간의 관계가 균등하다**는 점에서 적절한 표현 방식이라 할 수 있다.  

왜 그런지에 대한 이유를 살펴보자.  
여기서 Banana, Tomato, Apple라는 3개의 클래스가 존재하는 문제가 있다고 하면,  

정수 인코딩을 하여 1, 2, 3을 부여했고,  
MSE를 이용하여 Loss function을 계산하게 된다면 제곱 오차에 있어 오해를 불러일으킬 수 있다.  

Banana와 Tomato 사이의 오차보다 Banana와 Apple 사이의 오차가 더 크다.  
=>  
(2 - 1)^2 = 1  
(3 - 1)^2 = 4  

이는 컴퓨터에게 Banana가 Apple보다는 Tomato에 가깝다는 정보를 주게되어 문제점을 야기한다.  
(더 많은 클래스에 대해 문제가 될 수 있다. => 사용자가 부여하고자 했던 정보가 아니다.)  

클래스의 순서가 의미가 있는 것이 아니라면 클래스간의 오차는 균등해야 하는 것이 맞다.

그러므로, 정수 인코딩과 달리 One-Hot Enconding이 분류 문제의 클래스간의 관계를 균등하게 해준다.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126362969-3ba8848d-124f-4f41-85bb-5d676a2fd789.png" width = "300" ></p>  

### Softmax Regression  
이전 파트에서는 이진 분류를 하는 [Logistic Regression](https://lee-jaewon.github.io/deep_learning_study/Lec05/#1-%EA%B0%95%EC%9D%98-%EC%9A%94%EC%95%BD)에 대해 알아보았다.  

Softmax Regression을 통해 다중 클래스(Multi-Class Classification)를 분류하는 경우를 다루어보겠다.  

Softmax Regression도 Logistic Regression의 아이디어를 연장하여 적용한다.

Logistic Regression에서는 (WX + B)라는 Linear한 함수를 Sigmoid를 통해 0과 1의 구간으로 만들고,  
이를 Boundary Decision을 통해, 0과 1로 구분한다.
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126454722-368948ea-2712-450d-9bd5-2f75264ce174.png" width = "400" ></p> 

즉, Logisitic Regression으로 구현할 때 두 가지 경우의 확률의 합은 1이었다.

이 아이디어(확률의 총합이 1이라는 아이디어)를 Softmax Regression에 적용한다.  

Softmax Regression는 각 선택지(각 클래스)마다 소수의 확률의 할당하게 된다.  

결국, Softmax Regression는 선택지의 개수만큼의 차원을 가지는 벡터를 만들고, 

해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 원소들의 값을 변환시키는 어떤 함수를 지나게 만들어야 한다.

이를 Softmax 함수라 한다.

### Softmax Function
#### 1. Softmax function 이해하기  
k차원의 벡터에서 i번째 원소를 z_sub_i,  
i번째 클래스가 정답일 확률을 p_sub_i로 나타낼 때,  

p_sub_i를 다음과 같이 표현할 수 있다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126457314-266f42a9-6049-443d-bb36-2a82dd490568.png" width = "400" ></p>  

이것이 3개의 클래스를 풀어야하는 문제라면 k = 3이므로,  
3차원 벡터를 입력받아 다음과 같은 출력을 리턴한다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126457718-649873c4-a1fa-4b8c-bb31-09728360cf45.png" width = "400" ></p> 

다소 복잡해보이지만 분류하고자 하는 클래스가 k개일 때,  
k차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다는 내용이다.  

#### 2. Softmax function 이해하기(2)
자료에서 다루고 있는 예시인 꽃의 품종을 분류하기 위해 Softmax function을 적용하는 것을 다루어보겠다.  

분류해야하는 꽃의 개수는 K = 3, 즉 3개이고,  
꽃의 특성은 4개다.

**즉, 4개의 꽃의 특성과 이러한 특성을 지난 5개의 꽃들을 통해(샘플의 개수 = 5)**

**5개의 꽃들을 어떠한 꽃인지 3개로 분류해내는 것이 핵심이다.**  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126460206-1a38bcb4-c529-4dac-922e-826ffc862898.png" width = "400" ></p>  

그런데 소프트맥스의 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고자 하는 클래스의 개수가 되어야 하므로

어떤 가중치 연산을 통해 **3차원 벡터로 변환**되어야 한다.  







## 참고 자료
[Pytorch로 시작하는 딥러닝 입문](https://wikidocs.net/59427)  
[BoostCouse](https://www.boostcourse.org/ai214/lecture/42290?isDesc=false)  



