---
title : "Deep_Learning study Lec06"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Lec 06: Softmax Regression

## Boostcourse의 '파이토치로 시작하는 딥러닝 기초'를 통한 공부와 정리 Post

'파이토치로 시작하는 딥러닝 기초'와 '텐서플로우로 시작하는 딥러닝 기초'를 함께 공부하며 개념적인 부분에서 상호보완하고 있기 때문에 다른 포스트에서 같은 내용을 다루는 부분이 있을 수 있습니다.  

## Goal of Study
> - 다항 분류(Multinomial Classification)의 개념을 알아본다. 

### Keyword
> - 다항 분류(Multinomial Classification)   
> - 소프트맥스(Softmax)  
> - 크로스 엔트로피(Cross Entropy)  

## 1. 강의 요약  
### One-Hot Enconding
컴퓨터나 기계적 프로세스들은 문자보다 숫자를 더 잘 처리할 수 있다.  

그래서 문자를 숫자로 바꾸는 여러가지 기법 중 가장 기본적인 표현 방법이 One-Hot Enconding이다.

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다.  

이렇게 표현된 벡터를 **원-핫 벡터(One-Hot vector)**라고 한다.

예를 들어보자면,  
강아지, 고양이, 냉장고라는 3개의 선택지가 있을 때

강아지 = [1, 0, 0]  -> 0번 index  
고양이 = [0, 1, 0]  -> 1번 index  
냉장고 = [0, 0, 1]  -> 2번 index  

다음과 같이 정수로 부여된 인덱스를 원-핫 인코딩을 하게 되면 위와 같은 벡터를 가지게 된다.

꼭, One-Hot vector로 표현해야 다중 클래스 분류에 관한 문제를 풀 수 있는 것은 아니지만,  
다중 클래스 분류 문제가 각 **클래스 간의 관계가 균등하다**는 점에서 적절한 표현 방식이라 할 수 있다.

왜 그런지에 대한 이유를 살펴보자.  
여기서 Banana, Tomato, Apple라는 3개의 클래스가 존재하는 문제가 있다고 하면,  

정수 인코딩을 하여 1, 2, 3을 부여했고,  
MSE를 이용하여 Loss function을 계산하게 된다면 제곱 오차에 있어 오해를 불러일으킬 수 있다.

Banana와 Tomato 사이의 오차보다 Banana와 Apple 사이의 오차가 더 크다.  

이는 컴퓨터에게 Banana가 Apple보다는 Tomato에 가깝다는 정보를 주게되어 문제점을 야기한다.  
(더 많은 클래스에 대해 문제가 될 수 있다. => 사용자가 부여하고자 했던 정보가 아니다.)  

클래스의 순서가 의미가 있는 것이 아니라면 클래스간의 오차는 균등해야 하는 것이 맞다.

그러므로, 정수 인코딩과 달리 One-Hot Enconding이 분류 문제의 클래스간의 관계를 균등하게 해준다.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126362969-3ba8848d-124f-4f41-85bb-5d676a2fd789.png" width = "300" ></p>  

### Softmax Regression  


