---
title : "Deep_Learning study Lec06"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Lec 06: Softmax Regression  

## Boostcourse의 '파이토치, 텐서플로우로 시작하는 딥러닝 기초'를 통한 공부와 정리 Post    

## Goal of Study  
> - 다항 분류(Multinomial Classification)의 개념을 알아본다. 

### Keyword
> - 다항 분류(Multinomial Classification)   
> - 소프트맥스(Softmax)  
> - 크로스 엔트로피(Cross Entropy)  

## 1. 강의 요약  
### One-Hot Enconding
컴퓨터나 기계적 프로세스들은 문자보다 숫자를 더 잘 처리할 수 있다.  

그래서 문자를 숫자로 바꾸는 여러가지 기법 중 가장 기본적인 표현 방법이 One-Hot Enconding이다.

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다.  

이렇게 표현된 벡터를 **원-핫 벡터(One-Hot vector)**라고 한다.  
  
예를 들어보자면,  
강아지, 고양이, 냉장고라는 3개의 선택지가 있을 때  

강아지 = [1, 0, 0]  -> 0번 index  
고양이 = [0, 1, 0]  -> 1번 index  
냉장고 = [0, 0, 1]  -> 2번 index  

다음과 같이 정수로 부여된 인덱스를 원-핫 인코딩을 하게 되면 위와 같은 벡터를 가지게 된다.  

꼭, One-Hot vector로 표현해야 다중 클래스 분류에 관한 문제를 풀 수 있는 것은 아니지만,  
다중 클래스 분류 문제가 각 **클래스 간의 관계가 균등하다**는 점에서 적절한 표현 방식이라 할 수 있다.  

왜 그런지에 대한 이유를 살펴보자.  
여기서 Banana, Tomato, Apple라는 3개의 클래스가 존재하는 문제가 있다고 하면,  

정수 인코딩을 하여 1, 2, 3을 부여했고,  
MSE를 이용하여 Loss function을 계산하게 된다면 제곱 오차에 있어 오해를 불러일으킬 수 있다.  

Banana와 Tomato 사이의 오차보다 Banana와 Apple 사이의 오차가 더 크다.  
=>  
(2 - 1)^2 = 1  
(3 - 1)^2 = 4  

이는 컴퓨터에게 Banana가 Apple보다는 Tomato에 가깝다는 정보를 주게되어 문제점을 야기한다.  
(더 많은 클래스에 대해 문제가 될 수 있다. => 사용자가 부여하고자 했던 정보가 아니다.)  

클래스의 순서가 의미가 있는 것이 아니라면 클래스간의 오차는 균등해야 하는 것이 맞다.

그러므로, 정수 인코딩과 달리 One-Hot Enconding이 분류 문제의 클래스간의 관계를 균등하게 해준다.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126362969-3ba8848d-124f-4f41-85bb-5d676a2fd789.png" width = "300" ></p>  

### Softmax Regression  
이전 파트에서는 이진 분류를 하는 [Logistic Regression](https://lee-jaewon.github.io/deep_learning_study/Lec05/#1-%EA%B0%95%EC%9D%98-%EC%9A%94%EC%95%BD)에 대해 알아보았다.  

Softmax Regression을 통해 다중 클래스(Multi-Class Classification)를 분류하는 경우를 다루어보겠다.  

Softmax Regression도 Logistic Regression의 아이디어를 연장하여 적용한다.

Logistic Regression에서는 (WX + B)라는 Linear한 함수를 Sigmoid를 통해 0과 1의 구간으로 만들고,  
이를 Boundary Decision을 통해, 0과 1로 구분한다.
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126454722-368948ea-2712-450d-9bd5-2f75264ce174.png" width = "400" ></p> 

즉, Logisitic Regression으로 구현할 때 두 가지 경우의 확률의 합은 1이었다.

이 아이디어(확률의 총합이 1이라는 아이디어)를 Softmax Regression에 적용한다.  

Softmax Regression는 각 선택지(각 클래스)마다 소수의 확률의 할당하게 된다.  

결국, Softmax Regression는 선택지의 개수만큼의 차원을 가지는 벡터를 만들고, 

해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 원소들의 값을 변환시키는 어떤 함수를 지나게 만들어야 한다.

이를 Softmax 함수라 한다.

### Softmax Function
#### 1. Softmax function 이해하기  
k차원의 벡터에서 i번째 원소를 z_sub_i,  
i번째 클래스가 정답일 확률을 p_sub_i로 나타낼 때,  

p_sub_i를 다음과 같이 표현할 수 있다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126457314-266f42a9-6049-443d-bb36-2a82dd490568.png" width = "300" ></p>  

이것이 3개의 클래스를 풀어야하는 문제라면 k = 3이므로,  
3차원 벡터를 입력받아 다음과 같은 출력을 리턴한다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126457718-649873c4-a1fa-4b8c-bb31-09728360cf45.png" width = "500" ></p> 

다소 복잡해보이지만 분류하고자 하는 클래스가 k개일 때,  
k차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다는 내용이다.  

#### 2. Softmax function 이해하기(2)
자료에서 다루고 있는 예시인 꽃의 품종을 분류하기 위해 Softmax function을 적용하는 것을 다루어보겠다.  

분류해야하는 꽃의 개수는 K = 3, 즉 3개이고,  
꽃의 특성은 4개다.

**즉, 4개의 꽃의 특성과 이러한 특성을 지난 5개의 꽃들을 통해(샘플의 개수 = 5(n))**

**5(n)개의 꽃들을 어떠한 꽃인지 3개로 분류해내는 것이 핵심이다.**  
(샘플의 개수는 임의의 개수이다.)  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126460206-1a38bcb4-c529-4dac-922e-826ffc862898.png" width = "400" ></p>  

그런데 소프트맥스의 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고자 하는 클래스의 개수가 되어야 하므로

어떤 가중치 연산을 통해 **3차원 벡터로 변환**되어야 한다.  

#### 3. 행렬 연산으로 이해하기
꽃의 특성은 4개이고, 샘플의 개수는 5개라고 해보자.  

이 때, 행렬 X는 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126462451-41195878-4d1c-480f-98ff-906942bc35a3.png" width = "400" ></p>  

우리가 얻어야하는 예측값은 다음과 같다.  
3개의 꽃으로 분류해야하므로, 5 by 3의 행렬로 구성된다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126462740-7eea70f6-3d03-4f02-a641-8d91ffe738da.png" width = "400" ></p>

이 때, Weight는 5 by 4를 5 by 3으로 바꿔야하므로  
Weight는 4 by 3 이 된다는 것을 이해할 수 있을 것이다.  
[Weight에 대한 포스팅](https://lee-jaewon.github.io/deep_learning_study/Lec04(TensorFlow)/#%EC%B6%9C%EB%A0%A5%EC%9D%B4-%EC%97%AC%EB%9F%AC-%EA%B0%9C%EC%9D%B8-%EA%B2%BD%EC%9A%B0)  

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126463461-faccd4a4-2851-4996-968e-2cf2745eb01f.png" width = "400" ></p>

Bias는 벡터의 합 연산이므로, 예측값과 벡터의 크기가 동일해야한다.  
=> 5 by 3 

그러므로, 결과는 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126463642-14ae24aa-a86f-4591-8673-154a751a2303.png" width = "400" ></p>

이러한 과정을 통해, Softmax를 통해 나온 확률들을 One-hot 벡터로 업데이트하여

실제값을 추출한다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126463926-43e7f33e-ab81-4853-8dea-a1043f3f5be8.png" width = "400" ></p>  

### Cost Function  
Softmax Regression에서는 Cost Function으로 Cross Entropy함수를 사용한다.  

#### 1. Cross Entropy Function  

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126474759-2fbab6c0-6431-4e46-a5b6-0f2708fd1718.png" width = "300" ></p>  
크로스 엔트로피 함수를 다음과 같이 나타낸다.  

이 함수가 왜 적합한지를 알아보면,  

예를 들어, 만약 2번 index가 실제 값 One-hot 벡터에서 1을 가진 원소의 index라고 하면 

p_sub_2 = 1 은 예측 값이 실제 값을 정확히 예측한 경우이다.  
-1log(1) = 0 = cost  

즉, 간단한 설명만 하자면 식의 y_sub_j는 실제 값이며,  
p_sub_j는 예측값이다. 

원래의 예측 모형은 실제 값 q 분포(y)를 모르고 p를 이용하여 실제 분포를 예상하는 모델이다.

그래서 q와 p가 식에 모두 들어가 **Cross**-Entropy라고 하는 것이다.

머신러닝을 통한 예측 모델에서는 실제 분포 q를 알 수 있기 때문에 이를 계산할 수 있다.

즉, 훈련 데이터를 사용한 예측 모형에서 cross-entropy 는 실제 값과 예측값의 차이 (dissimilarity) 를 계산하는데 사용할 수 있다는 것이다. 

어쨋든, 우리는 이 함수를 최소화하는 방향으로 학습을 시켜야한다.  

#### 2. Cross-Entropy for Binary Classification  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126290252-43a47e62-add6-47c4-b783-50441d39f289.png" width = "400" ></p>  
이진 분류에서 다루었던 Cost function이다.

위의 식에서 y를 y1으로 치환하고   
1 - y을 -y2로 치환한다.  

H(x)는 p1, 1-H(x)는 p2로 치환하면,  
결과적으로 아래의 식을 얻을 수 있다.    
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/126474335-c341add4-e5c8-4a50-afc8-ad38eb77684a.png" width = "400" ></p>  

결국, 같다는 것을 확인할 수 있다.  












## 참고 자료
[Pytorch로 시작하는 딥러닝 입문](https://wikidocs.net/59427)  
[BoostCouse](https://www.boostcourse.org/ai214/lecture/42290?isDesc=false)  



