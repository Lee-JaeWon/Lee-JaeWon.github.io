---
title : "Lec 09: ReLU"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Deep_Learning study Lec09-1  

## Boostcourse의 '파이토치로 시작하는 딥러닝 기초'를 통한 공부와 정리 Post    

## Goal of Study  
> - ReLU 활성화 함수에 대해 알아본다. 

### Keyword
> - ReLU  
> - Sigmoid  
> - Optimizer   

## 1. 강의 요약  
### Problem of Sigmoid  
우리가 지금까지 사용해 왔던 Activation Function 중 Sigmoid 문제점이 무엇일까?  

Sigmoid 함수의 형태를 떠올려보자.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127188269-8af0336a-4b5c-4a56-9f2d-8d4e0085f4a1.png" width = "400" ></p>  

우리가 [BackPropagation](https://lee-jaewon.github.io/deep_learning_study/Lec08/)을 수행할 때, Sigmoid 함수의 문제점이 나타난다.  

역전파의 과정에서 순전파(Forward Propagation)를 통한 Error(Loss)에  
W가 미치는 영향을 알기 위해 sigmoid 함수를 미분하는 과정을 거치게 된다.    

이때, 위 그림에서 붉은 부분은 Gradient가 0에 거의 가까운 값의 형태가 나타나게 되는데,  
이것이 역전파를 수행하는 과정에서 이 기울기가 곱해지는 형태이다 보니  
결과적으로 **기울기 소실(Vanishing Gradient)**이 발생하는 현상이 일어난다.  

그러면 앞 단 까지 기울기가 잘 전달되지 않기 때문에, 은닉층이 개수가 더 늘어남에 따라 전파가 안되며  
W가 업데이트 되지 않아 학습이 되지 않는다.  

### ReLU
인공 신경망에서 가장 최고의 인기를 얻고 있는 함수로 알려져있다.  
수식은 $ f(x) = max(0, x) $의 형태로 간단하다. 

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127190257-2836e5c2-933d-4b22-9e3f-c7367e569998.png" width = "300" ></p>  

**ReLU**함수는 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값 그대로 반환한다. 

이 함수는 Sigmoid와 달리 특정 양수값에 수렴하지 않으므로 훨씬 더 잘 작동한다.

시그모이드 함수와 하이퍼볼릭탄젠트 함수와 같이 어떤 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠른것이 특징이다.

하지만, 문제점은 입력값이 음수면 기울기도 0이 되기 때문에, 같은 문제가 발생하며  
이 뉴런을 다시 회생하는 것이 매우 어려워진다.  

이를 죽은 **렐루(dying ReLU)**라고 한다.  

Pytorch에서는 sigmoid 대신에 다음과 같이 사용할 수 있다.  
```python
x = torch.nn.relu(x)
```

#### +Leaky ReLU
죽은 렐루를 보완하기 위해 ReLU의 변형 함수들이 등장했고,

그 중 Leaky ReLU는 입력값이 음수인 경우, 0이 아니라 0.001 처럼 매우 작은 수를 반환하도록 한다.

수식은 % f(x) = max(ax, x) $로 또한 간단하며, 다음과 같은 형태이다.  
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127191824-ca6ba827-0de4-4642-8298-e94c446a627c.png" width = "300" ></p>

위와 같이 입력값이 음수라도 기울기가 0이 되지 않으면 ReLU는 죽지 않는다.

### Optimizer in PyTorch
```python
torch.optim.SGD
torch.optim.Adadelta
torch.optim.Adagrad
torch.optim.Adam
torch.optim.SparseAdam
torch.optim.Adamax
torch.optim.ASGD
torch.optim.LBFGS
torch.optim.RMSprop
torch.optim.Rprop
```
 위처럼 PyTorch에서 `torch.optim`이라는 Optimizing을 위한 다양한 Optimization 알고리즘을 제공한다.  

추후에 다뤄볼 기회가 더 있으면 좋을 것 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127192605-8ed9e9f4-160f-4a28-a382-93637bde1fe1.png" width = "500" ></p>

📣<br>
본 포스팅의 학습 환경 : `Anaconda`, `CPU`, `Pytorch`, `Jupyter Notebook`  
포스팅에서 오류나 궁금한 점은 Comments를 작성해주시면, 많은 도움이 됩니다.💡
{: .notice--info}