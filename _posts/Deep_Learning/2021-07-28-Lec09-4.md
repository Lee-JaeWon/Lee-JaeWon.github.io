---
title : "Lec 09-4: Batch Normalization"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
    - Boost Course
toc : true
toc_sticky: true
comments: true
---

Deep_Learning study Lec09-4 

## Boostcourseì˜ 'íŒŒì´í† ì¹˜ë¡œ ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ì´ˆ'ë¥¼ í†µí•œ ê³µë¶€ì™€ ì •ë¦¬ Post    

## Goal of Study  
> - Batch Normalization ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.  

### Keyword
> - Batch Normalization  
> - ê²½ì‚¬ ì†Œì‹¤(Gradient Vanishing) / í­ë°œ(Exploding)      

## 1. ê°•ì˜ ìš”ì•½  
### Gradient Exploding
Gradient Vanishingê³¼ ë°˜ëŒ€ë¡œ gradientë¥¼ ì• ë‹¨ìœ¼ë¡œ ì „íŒŒí•  ë•Œ, ì´ ê°’ì´ ì˜¤íˆë ¤ ë„ˆë¬´ í° ê°’ì´ ë˜ì–´ë²„ë¦¬ëŠ” ê²½ìš°ë¥¼ Gradient Explodingì´ë¼ê³  í•œë‹¤.

### Solution
> - Change activation function  
> - Careful initialization  
> - Small learning rate  

ìœ„ëŠ” ê°„ì ‘ì ì¸ í•´ê²° ë°©ì‹ì´ê³ ,

ìš°ë¦¬ëŠ” ì§ì ‘ì ì¸ í•´ê²° ë°©ì‹ì¸ **Batch Normalization**ì´ë¼ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.

### Internal Covariate Shift(ICS)
ICSê°€ Gradient Explodingì´ë‚˜ Vanishing ê°™ì€ ë¬¸ì œë¥¼ ë§Œë“ ë‹¤ê³  Batch Normalization ì €ìë“¤ì€ ì£¼ì¥í•œë‹¤.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127341161-132d07d2-7ec9-4e92-b3f5-4011c5036231.png" width = "300" ></p> 

ë¨¼ì €, Covariate Shiftë€ Train-setê³¼ Test-setì˜ ë¶„í¬ê°€ ì‹¤ì œë¡œ ì°¨ì´ê°€ ìˆê³ ,  
ì´ ë¶„í¬ì˜ ì°¨ì´ê°€ ì–´ë– í•œ ë¬¸ì œì ì„ ë°œìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ Covariate Shiftì˜ ê°œë…ì´ë‹¤.

ì¦‰, ë°ì´í„° ë¶„í¬ì— ë³€í™”ê°€ ìƒê¸´ë‹¤ëŠ” ê²ƒì´ë‹¤.

> ê·¸ë ‡ë‹¤ë©´ ICSì™€ NN(Neural Network)ì—ì„œëŠ” ICSì˜ ë¬¸ì œì ì´ ì–´ë–»ê²Œ ë°œìƒí•˜ëŠ”ì§€ ì•Œì•„ë³´ì.

ìš°ë¦¬ê°€ ë°ì´í„°ë¥¼ í•™ìŠµí•  ë•Œ, Neural Network ëª¨ë¸ì—ì„œ Lossë¥¼ ê³„ì‚°í•˜ê³  ì´ë¥¼ Back Propagationì„ í†µí•´ ì´ Networkë¥¼ ì—…ë°ì´íŠ¸ í•˜ê²Œ ëœë‹¤.

ì´ëŸ° ìƒí™©ì—ì„œ ì´ì œ ICSë¼ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.

Neural Networkì˜ ëª¨ë¸ì—ì„œ í•™ìŠµì‹œ ì´ ê³¼ì •ì—ì„œ layerë¥¼ ì§€ë‚ ë•Œë§ˆë‹¤, ì…ë ¥ì˜ ë¶„í¬ê°€ ê³„ì† ë³€í•˜ê²Œ ëœë‹¤.
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127342907-5b470f7b-f634-49a6-b21a-f29f3dea268a.png" width = "400" ></p> 
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127342796-d9c38883-3efe-4d95-ab92-2b9802940fcf.png" width = "400" ></p> 

ê²°êµ­ì—ëŠ” í•œ layerë§ˆë‹¤ Covariate Shiftê°€ ë°œìƒí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ í˜„ìƒì„ **Internal Covariate Shift**ë¼ê³  í•œë‹¤.

### Batch Normalization
ReLU ê³„ì—´ì˜ í•¨ìˆ˜ì™€ He ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì–´ëŠ ì •ë„ ê¸°ìš¸ê¸° ì†Œì‹¤ê³¼ í­ì£¼ë¥¼ ì™„í™”ì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ,  
ì´ ë‘ ë°©ë²•ì„ ì‚¬ìš©í•˜ë”ë¼ë„ í•™ìŠµ ì¤‘ì— ì–¸ì œë“  ë¬¸ì œê°€ ë‹¤ì‹œ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

Batch Normalizationì€ ì¸ê³µ ì‹ ê²½ë§ì˜ ê° ì¸µì— ë“¤ì–´ê°€ëŠ” ì…ë ¥ì„ í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ í•™ìŠµì„ íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“ ë‹¤.

ì¦‰, ê° layerë§ˆë‹¤ Normalizationì„ í•˜ëŠ” layerë¥¼ ë‘ì–´ì„œ ë³€í˜•ëœ ë¶„í¬ê°€ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.

ì´ë¥¼, ìš°ë¦¬ê°€ Neural Networkë¥¼ í•™ìŠµí•  ë•Œ, mini-batchë¡œ ìª¼ê°œì„œ í•™ìŠµì„ í•˜ëŠ” ì¼ë°˜ì ì¸ ê²½ìš°ì—  
ì´ mini-batchë“¤ë§ˆë‹¤ Normalizationì„ í•´ì£¼ê² ë‹¤ëŠ” ì˜ë¯¸ì—ì„œ **Batch Normalization**ë¼ê³  í•œë‹¤.

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127344939-1316e813-40f5-4e2c-b4cc-89000f25d1b5.png" width = "400" ></p> 

ìœ„ì˜ ì•Œê³ ë¦¬ì¦˜ì´ Batch Normalizationì´ë¼ê³  í•˜ëŠ” ë°©ì‹ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì´ë‹¤.

Input : ë¯¸ë‹ˆ ë°°ì¹˜ $B = {x^{(1)}, x^{(2)},...,x^{(m)}}$

ë°°ì¹˜ ì •ê·œí™”ëŠ” **í•™ìŠµ ì‹œ** ë°°ì¹˜ ë‹¨ìœ„ì˜ í‰ê· ê³¼ ë¶„ì‚°ë“¤ì„ ì°¨ë¡€ëŒ€ë¡œ ë°›ì•„ ì´ë™ í‰ê· ($\mu$)ê³¼ ì´ë™ ë¶„ì‚°(variance, $\sigma$)ì„ ë”°ë¡œ ì €ì¥í•´ë†“ëŠ”ë‹¤.

ì´ë¥¼ Sample mean, Sample Varianceì´ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.

ì´ë“¤ì˜ í‰ê· ì„ ì´ìš©í•´ Learning meanê³¼ Learning Varianceì„ ê³„ì‚°ì„ í•˜ê³ , ì´ ë‘˜ì€ Batch Normalizationì˜ í•™ìŠµì´ ëë‚˜ê²Œ ë˜ë©´ ì…ë ¥ batch ë°ì´í„°ì— ìƒê´€ì´ ì—†ì´ ê³ ì • ê°’ì´ ëœë‹¤.

í…ŒìŠ¤íŠ¸ í•  ë•ŒëŠ” í•´ë‹¹ ë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•˜ì§€ ì•Šê³  êµ¬í•´ë†“ì•˜ë˜ í‰ê· (Learning mean)ê³¼ ë¶„ì‚°(Learning Variance)ìœ¼ë¡œ ì •ê·œí™”ë¥¼ í•œë‹¤.  
(ì´ì „ì— ë‹¤ë£¬ Train & eval modeì— ê´€í•œ ë‚´ìš©)

### mnist_batchnorm
ê³„ì†í•´ì„œ MNISTí•™ìŠµ ê³¼ì •ì—ì„œ ê°œì„ í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì½”ë“œì— ìˆì–´ ìš°ë¦¬ê°€ ë‹¤ë¤„ë³¸ í•™ìŠµ ë°©ì‹ í˜¹ì€ ê°œì„  ë°©ì‹ì— ëŒ€í•´  
ì–´ëŠ ë¶€ë¶„ì´ ê°œë…ì— ë”°ë¼ ì‹¤ì œë¡œ ì½”ë“œê°€ ë‹¬ë¼ì§€ëŠ”ì§€ë¥¼ ë‹¤ë¤„ë³´ê³  ìˆë‹¤.

mnist_batchnormì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë‹¬ë¼ì§€ëŠ” ì‚¬í•­ì€ ì•„ë˜ì™€ ê°™ë‹¤.

Batch Normalizationì„ ì ìš©í•œ ê²ƒê³¼ ì ìš©í•˜ì§€ ì•Šì€ ê²ƒì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸° ìœ„í•œ ì½”ë“œì´ë‹¤.
```python
# nn layers
# Batchnorm ì‚¬ìš©
linear1 = torch.nn.Linear(784, 32, bias=True)
linear2 = torch.nn.Linear(32, 32, bias=True)
linear3 = torch.nn.Linear(32, 10, bias=True)
relu = torch.nn.ReLU()
bn1 = torch.nn.BatchNorm1d(32)
bn2 = torch.nn.BatchNorm1d(32)

# Batchnorm ì‚¬ìš©x
nn_linear1 = torch.nn.Linear(784, 32, bias=True)
nn_linear2 = torch.nn.Linear(32, 32, bias=True)
nn_linear3 = torch.nn.Linear(32, 10, bias=True)

# model
bn_model = torch.nn.Sequential(linear1, bn1, relu,
                            linear2, bn2, relu,
                            linear3).to(device)
nn_model = torch.nn.Sequential(nn_linear1, relu,
                            nn_linear2, relu,
                            nn_linear3).to(device)
```

#### Full code
ì „ì²´ ì½”ë“œëŠ” ê°•ì˜ì—ì„œ ì œê³µí•˜ëŠ” Codeë¥¼ ëŒ€ë¶€ë¶„ ì°¸ê³ í•´ì„œ í•™ìŠµí•´ë³´ê³ , í•„ìš”í•˜ë‹¤ë©´ ë”°ë¼ì„œ ì½”ë”©í•´ë³¸ ë³¸ì¸ ì½”ë“œë¥¼ í¬ìŠ¤íŒ…í•œë‹¤.  
[Full Code](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-09_6_mnist_batchnorm.ipynb)ë¥¼ ì°¸ê³ í•˜ë©´ ë„ì›€ì´ ë  ê²ƒ ê°™ë‹¤.

ë”°ë¡œ ì½”ë“œëŠ” í¬ìŠ¤íŒ…í•˜ì§€ ì•Šê³  í•™ìŠµ ê²°ê³¼ë§Œ í¬ìŠ¤íŒ…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
```
[Epoch 1-TRAIN] Batchnorm Loss(Acc): bn_loss:0.12966(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.18779(nn_acc:0.94)
[Epoch 1-VALID] Batchnorm Loss(Acc): bn_loss:0.14066(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.20529(nn_acc:0.94)

[Epoch 2-TRAIN] Batchnorm Loss(Acc): bn_loss:0.09642(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.16732(nn_acc:0.95)
[Epoch 2-VALID] Batchnorm Loss(Acc): bn_loss:0.11711(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.18843(nn_acc:0.94)

[Epoch 3-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08405(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.15942(nn_acc:0.96)
[Epoch 3-VALID] Batchnorm Loss(Acc): bn_loss:0.11109(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.20730(nn_acc:0.95)

[Epoch 4-TRAIN] Batchnorm Loss(Acc): bn_loss:0.08102(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.14705(nn_acc:0.96)
[Epoch 4-VALID] Batchnorm Loss(Acc): bn_loss:0.12212(bn_acc:0.96) vs No Batchnorm Loss(Acc): nn_loss:0.18678(nn_acc:0.95)

[Epoch 5-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06202(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12650(nn_acc:0.96)
[Epoch 5-VALID] Batchnorm Loss(Acc): bn_loss:0.09171(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.17650(nn_acc:0.96)

[Epoch 6-TRAIN] Batchnorm Loss(Acc): bn_loss:0.06397(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12816(nn_acc:0.96)
[Epoch 6-VALID] Batchnorm Loss(Acc): bn_loss:0.10242(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.19704(nn_acc:0.95)

[Epoch 7-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05521(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12984(nn_acc:0.96)
[Epoch 7-VALID] Batchnorm Loss(Acc): bn_loss:0.09313(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.19197(nn_acc:0.96)

[Epoch 8-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05698(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.12324(nn_acc:0.97)
[Epoch 8-VALID] Batchnorm Loss(Acc): bn_loss:0.10201(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.18410(nn_acc:0.96)

[Epoch 9-TRAIN] Batchnorm Loss(Acc): bn_loss:0.05078(bn_acc:0.98) vs No Batchnorm Loss(Acc): nn_loss:0.11740(nn_acc:0.97)
[Epoch 9-VALID] Batchnorm Loss(Acc): bn_loss:0.09656(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.19621(nn_acc:0.96)

[Epoch 10-TRAIN] Batchnorm Loss(Acc): bn_loss:0.04743(bn_acc:0.99) vs No Batchnorm Loss(Acc): nn_loss:0.12933(nn_acc:0.96)
[Epoch 10-VALID] Batchnorm Loss(Acc): bn_loss:0.09120(bn_acc:0.97) vs No Batchnorm Loss(Acc): nn_loss:0.21004(nn_acc:0.95)

Learning finished
```

<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127354611-5193387a-751f-4956-bcb2-4f491fcc38c6.png" width = "500" ></p> 
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127354616-3c8f2f99-0553-48a8-b0b6-b463b36b63b5.png" width = "500" ></p> 
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127354620-22cfa65c-ffb2-4e33-b138-8138a49258f3.png" width = "500" ></p> 
<p align="center"><img src="https://user-images.githubusercontent.com/72693388/127354624-59fd6527-2687-4cef-b37f-6fb3b08b1498.png" width = "500" ></p> 

ğŸ“£<br>
ë³¸ í¬ìŠ¤íŒ…ì˜ í•™ìŠµ í™˜ê²½ : `Anaconda`, `CPU`, `Pytorch`, `Jupyter Notebook`  
í¬ìŠ¤íŒ…ì—ì„œ ì˜¤ë¥˜ë‚˜ ê¶ê¸ˆí•œ ì ì€ Commentsë¥¼ ì‘ì„±í•´ì£¼ì‹œë©´, ë§ì€ ë„ì›€ì´ ë©ë‹ˆë‹¤.ğŸ’¡
{: .notice--info}