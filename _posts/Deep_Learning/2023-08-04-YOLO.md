---
title : "[논문 리뷰] You Only Look Once: Unified, Real-Time Object Detection (YOLO)"
category :
    - Deep_Learning_Study
tag :
    - Deep_Learning
toc : true
toc_sticky: true
comments: true
sidebar_main: true
---
[CVPR, 2016] You Only Look Once: Unified, Real-Time Object Detection (YOLO) 논문 리뷰

# Abstract
본 논문에서는 object detection에 대한 새로운 접근 방식인 **YOLO**를 제시한다.

이전의 object detection에 대한 연구들은 분류기를 다시 사용하여 감지를 수행했다.

하지만 본 논문은 object detection을 공간적으로 분리된 bounding box 및 관련된 class 확률들로의 회귀 문제로 정의한다.

단일 신경망은 **한 번의** 평가로부터 직접적으로 전체 이미지에서 바운딩 박스와 클래스 확률들을 예측한다.

전체 detection 파이프라인이 **하나의 네트워크로 통합**되어 있기 때문에 감지 성능에 대해 최적화를 직접적으로 수행할 수 있다.

또한 속도도 극도로 빠르다고 소개한다.<br>
(초당 45프레임의 속도로 실시간 이미지 처리를 수행한다. 더 작은 버전인 Fast YOLO은 초당 155프레임의 놀라운 속도로 작동하며 다른 실시간 감지기보다 2배 높은 mAP를 달성하였다.)

SOTA detction system과 비교했을 때, YOLO는 더 많은 localization error가 발생하지만,<br>
이것을 false positive(FP-맞는 것을 틀리다함.)로 예측할 가능성은 적다.

자연 이미지에서 다른 예술 작품과 같은 다른 도메인으로 일반화하는 경우, DPM과 R-CNN을 포함한 다른 감지 방법들보다 뛰어난 성능을 보인다.

# Introduction
현재의 감지 시스템들은 감지를 수행하기 위해 분류기(classifier)를 사용한다.<br>
객체를 감지하기 위해 이러한 시스템은 해당 객체에 대한 분류기를 가져와 테스트 이미지의 다양한 위치와 스케일에서 평가한다.

(DPM)과 같은 시스템은 분류기를 전체 이미지에 고르게 분포된 위치에서 실행하는 [슬라이딩 윈도우 접근법](https://lee-jaewon.github.io/deep_learning_study/faster_r_cnn/#region-proposal-networks)을 사용한다.

R-CNN과 같은 더 최근의 접근 방식은 먼저 이미지에서 잠재적인 바운딩 박스를 생성하기 위해 region proposal 방법을 사용한 다음<br>
제안된 박스에서 분류기를 수행한다.<br>
([R-CNN 포스팅](https://lee-jaewon.github.io/deep_learning_study/r_cnn/))

분류 후에 bounding box에 관한 후 처리와 같은 작업들로 인해 파이프라인이 복잡하고,<br>
각 개별 구성 요소를 별도로 훈련해야하기 때문에 최적화하기 어렵다.

**본 논문은** object detection을 이미지 픽셀에서 직접적으로 바운딩 박스 좌표와 클래스 확률로의 단일 회귀 문제로 정의한다.

이미지를 한 번만 보면(YOLO) 존재하는 물체와 그 위치를 예측할 수 있다.

<p align="center"><img src="/MyPDF/yolo(1).png" width = "500" ></p>
<p align="center">Figure 1</p>

Figure 1처럼 **YOLO는 간단**하다.

Single convolution network가 여러 개의 바운딩 박스와 해당 박스들에 대한 클래스 확률들을 **동시에** 예측한다.

YOLO는 전체 이미지에서 훈련을 수행하고 감지 성능을 직접적으로 최적화한다.

기존의 객체 감지 방법보다 여러 가지 이점이 존재한다.

**첫 번째**로, YOLO는 극도로 **빠르다.**<br>
감지를 회귀 문제로 정의하기 때문에 복잡한 파이프라인이 필요하지 않다.

배치 처리 없이 초당 45프레임으로 작동하며, 빠른 버전은 초당 150fps 이상의 속도로 작동한다.<br>
또한 mAP를 2배 이상 달성하였다.

**두 번째**로,<br>
YOLO는 예측을 수행할 때 이미지에 대해 **전역적으로 추론**한다.

슬라이딩 윈도우와 region proposal 기반 기술과 달리 YOLO는 훈련 및 테스트 시에 전체 이미지를 볼 수 있으므로 클래스와 그 모습에 대한 문맥적 정보를 암묵적으로 인코딩하게 된다.

Top detectio method인 Fast R-CNN은 이미지의 배경 패치를 객체로 잘못 인식하는 경우가 발생한다.

이는 더 큰 문맥을 볼 수 없기 때문이다.<br>
YOLO는 Fast R-CNN과 비교하여 배경 오류의 개수가 절반 이하이다.

**세 번째**로,<br>
YOLO는 객체의 일반화 가능한 표현을 학습한다.

자연 이미지에서 훈련된 후 예술 작품에서 테스트하는 경우, YOLO는 DPM과 R-CNN과 같은 최고의 감지 방법들보다 큰 폭으로 우월한 성능을 보인다.

YOLO는 굉장한 일반화가 가능하기 때문에 새로운 도메인이나 예상치 못한 입력에 적용할 때 더 적은 결함이 발생할 가능성이 높다.

YOLO는 여전히 SOTA detection system에 비해 **정확성에서는 뒤쳐지고 있다.**

빠르게 물체를 식별할 수는 있지만 특히 작은 물체를 정확하게 지역화하는 데 어려움이 있다.

# 2. Unified Detection
Object Detection의 각 구성 요소를 **하나의 신경망으로** 통합한다.

신경망은 전체 이미지의 특성을 사용하여 각 bounding box를 예측한다.

또한 이미지 전체와 모든 객체에 대해 동시에 모든 bounding box를 예측한다.<br>
(이는 신경망이 이미지 전체와 이미지 내의 모든 객체에 대해 전역적으로 추론하는 것을 의미)

YOLO 디자인은 **end-to-end** 학습과 real-time 속도를 가능하게 하면서 높은 평균 정확도를 유지한다.

시스템은 입력 이미지를 $S \times S$ 그리드로 나눈다.

객체의 **중심이** grid cell 안에 떨어지면(fall into) 해당 grid cell은 해당 객체를 탐지하는 책임을 진다.

각 grid cell은 bounding box $B$와 해당 box들의 신뢰도 점수를 예측한다.

이 신뢰도 점수는 모델이 상자가 객체를 포함하고 있을 확률과 예측한 상자의 정확성을 나타낸다.

신뢰도 점수는 다음과 같이 정의한다.<br>
$ Pr(Object) * IOU^{truth}_{pred} $

grid cell에 아무 객체가 없으면 $ Pr(Object) = 0 $ 이다. 그러면 신뢰도 점수도 0이 된다.

그렇지 않으면 predicted box와 ground-truth 사이의 교차면적 비율(IOU)과 같아야 한다.

---

각 bounding box는 5개의 예측값으로 구성된다.

x,y,w,h와 신뢰도(confidence)이다.

(x, y) 좌표는 grid cell 경계를 기준으로 상자의 중심이며,

너비(w)와 높이(h)는 전체 이미지를 기준으로 예측된다.

마지막으로 신뢰도(confidence) 예측은 예측한 상자와 실제 ground truth 상자 사이의 **IOU**를 나타낸다.

각 grid cell은 또한 조건부 클래스 확률 $C$를 예측한다.<br>
$C=Pr(Class_i \mid Object)$

이러한 확률은 grid cell이 객체를 포함하는 경우에 그 객체가 어떤 class인지에 대한 조건부 확률이다.

하나의 grid cell 당 하나의 class 확률 값만 예측한다.

이 말은 상자의 개수 B와는 관계없이 하나의 grid cell에 대해 하나의 클래스 확률을 예측한다는 뜻이다.

테스트 시에는 conditional class probability(C)와 individual box confidence 예측값을 곱하여 다음과 같은 값을 얻는다.

$Pr(Class_i \mid Object) * Pr(Object) * IOU^{truth}_{pred}$

$= Pr(Class_i) * IOU^{truth}_{pred}$

위 식은 각 상자에 대한 클래스별 신뢰도 점수를 제공한다.

이 점수는 상자에 해당 클래스가 나타날 확률과 예측한 상자가 객체와 얼마나 잘 맞는지를 함께 인코딩한다.

<p align="center"><img src="/MyPDF/yolo(2).png" width = "500" ></p>
<p align="center">Figure 2</p>

PASCAL VOC에서 evaluating하기 위해 $S=7$, $B=2$를 사용했고,

PASCAL VOC는 20개의 labelled class가 있기에 $C = 20$이다.

최종 예측 tensor는 $7 \times 7 \times 30$ 이다.

($B=2$는 하나의 grid cell에서 2개의 bonding box를 예측한다는 것이다.)

## 2.1. Network Design
**Convolution neural network**로 구현하고, PASCAL VOC detection dataset에서 평가한다.

network 앞 단의 convolution layer는 이미지에서 특성을 추출하고 FC layer는 확률과 좌표를 예측한다.

본 논문의 network architecture는 GoogLeNet 모델에서 영감을 얻었다.

24개의 convolutional layer와 그 뒤 2개의 FC layer로 구성된다.

GoogLeNet의 inception 모듈 대신 1x1 reduction layer와 그 뒤에 3x3 convolutional layer를 사용한다.

<p align="center"><img src="/MyPDF/yolo(3).png" width = "700" ></p>
<p align="center">Figure 3</p>

또한 빠른 객체 탐지의 한계를 끌어올리기 위해 **Fast YOLO**라는 빠른 버전의 YOLO를 훈련시켰다.

Fast YOLO는 더 적은 합성곱 레이어(24개 대신 9개)와 레이어에서 더 적은 필터를 사용하는 신경망을 사용한다.

네트워크 크기를 제외한 모든 학습과 테스트 매개변수는 YOLO와 Fast YOLO 사이에 동일하다.

네트워크의 최종 출력은 7x7x30 크기의 텐서로 구성된다.

## 2.2. Training
ImageNet 1000-class competition 데이터셋에서 convolutional layer를 pre-train시킨다.

pretraining을 위해 Figure 3의 처음 20개의 convolutional layer만을 사용하고 average pooling 레이어와 FC layer를 더한다.

이 네트워크를 약 1주일 동안 훈련하고 ImageNet 2012 검증 데이터셋에서 88%의 top-5 정확도를 달성하였다.

모든 훈련과 추론은 Darknet 프레임워크를 사용한다.

그 후, 모델을 **detection을 수행하도록 변환한다.**

다른 연구에서 사전 훈련된 네트워크 뒤에 무작위로 초기화된 가중치로 4개의 컨볼루션 계층 및 2개의 전결합 계층을 추가하여 성능을 향상시켰다.

Detection에서 세밀한 시각 정보를 요구할 수도 있어 입력 resolution을 224x224에서 448x448로 증가시켰다.

최종 레이어는 클래스 확률과 bounding box 좌표를 모두 예측한다.

Bounding box의 너비와 높이는 이미지의 너비와 높이로 정규화하여 0과 1 사이로 만든다.

Bounding box x와 y 좌표를 특정 grid cell 위치의 오프셋으로 매개변수화하를 지만, 역시 0과 1 사이에 놓이도록한다.(좌상 좌표계, 중심 좌표계 등등 처럼?)

마지막 레이어에는 선형 활성화 함수를 사용하며, 다른 모든 레이어에서는 다음과 같은 Leaky ReLU 활성화 함수를 사용한다.

<p align="center"><img src="/MyPDF/yolo(4).png" width = "400" ></p>

---

본 논문은 모델의 출력값에 대해 평균 제곱 오차(sum-squared error)를 최적화한다.

평균 제곱 오차를 사용하는 이유는 최적화가 쉽기 때문이다.

하지만 이것은 Average precision를 최대화하는 본 논문의 목표와 완벽하게 일치하지는 않다.

평균 제곱 오차는 localization 오차와 classification 오차를 동등하게 가중시키기 때문에 이상적이지 않을 수 있다.

또한, 모든 이미지에서 **많은 grid cell이 객체를 포함하지 않을 수 있다.**

이로 인해 해당 셀의 "신뢰도" 점수가 0에 가까워지는데, 이는 종종 객체를 포함하는 셀로부터의 기울기를 압도할 수 있다.(즉 confidence score가 0이 되도록 학습하게 된다.)

이는 **모델의 불안정성을 초래**하고, **훈련이 일찍 발산하는 원인**이 될 수 있다.

이를 해결하기 위해, bounding box 좌표 예측의 loss를 증가시키고, 객체를 포함하지 않는 상자에 대한 신뢰도 예측의 loss를 감소시킨다.

이를 위해 두 파라미터 $\lambda_{coord}$ 와 $\lambda_{noobj}$ 를 사용한다.

$\lambda_{coord} = 5$ , $\lambda_{noobj} = 0.5$

또한 SSE(sum-squared error)는 큰 bounding box와 작은 boudning box에 대해 모두 동일한 가중치로 loss를 계산한다.

하지만 본 논문의 오차 측정은 큰 상자에서의 작은 편차가 작은 상자에서의 편차보다 중요하지 않음을 반영해야 한다.

이를 개선하기 위해 bounding box의 너비(widht)와 높이(hegith)에 square root를 취해주었다.

너비와 높이에 square root를 취해주면 너비와 높이가 커짐에 따라 그 증가율이 감소해 loss에 대한 가중치를 감소시키는 효과가 있기 때문이다.<br>
([출처 : 논문 리뷰 - YOLO(You Only Look Once) 톺아보기](https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once))

---

YOLO는 각 grid cell마다 여러 개를 bounding하지만, box를 예측한다.

훈련 시에는 하나의 객체를 예측하는 데 책임을지는 bounding box predictor가 하나만 있으면 된다.

본 논문은 현재 ground truth와 가장 높은 IOU를 가진 예측을 기준으로 하나의 예측기를 "responsible" 하게 지정한다.

이로 인해 각 예측기는 특정 크기, 종횡비 또는 클래스의 객체를 더 잘 예측하게 되며, 전반적인 검출률을 향상시킨다.

---

다음과 같은 다중 파트 손실 함수를 최적화한다.

<p align="center"><img src="/MyPDF/yolo(5).png" width = "500" ></p>

여기서,<br>
$1^{obj}_{i}$ 는 grid cell $i$ 안에 객체가 존재하는지의 여부를 의미한다.<br>
(존재하면 1, 아니면 0)

$1^{obj}_{i \ j}$ 는 grid cell $i$의 $j$번째 bounding box predictor가 사용되는지 여부를 의미한다.

loss function은 해당 grid cell에 **객체가 있는 경우에만** classification error에 패널티를 부여한다.<br>
(앞서 설명한 조건부 클래스 확률 때문)

또한 해당 예측기가 실제 상자에 "책임"이 있는 경우에만 bounding box 좌표 오차에 벌칙을 부과한다.<br>
(해당 grid cell 내의 어떤 예측기보다 높은 IOU를 가지는 경우)

---

정리하자면 식 5개에 대해<br>
**(1)** object가 존재하는 grid cell $i$에서 bounding box predictor $j$에 대해 중심 좌표의 loss 계산

**(2)** object가 존재하는 grid cell $i$에서 bounding box predictor $j$에 대해, square root된 width와 height에 대한 loss를 계산

**(3)** object가 존재하는 grid cell $i$에서 bounding box predictor $j$에 대해, confidence score의 loss를 계산

**(4)** object가 존재하지 않는 grid cell $i$에서 bounding box predictor $j$에 대해, confidence score의 loss를 계산

**(5)** object가 존재하는 grid cell $i$에 대해, conditional class probability의 loss를 계산

이외 $\lambda_{coord}$ 와 $\lambda_{noobj}$는 위에서 설명했듯이 balancing을 위한 파라미터.

## 2.3. Inference
훈련과 마찬가지로, 테스트 이미지에 대한 detection을 예측하는 것은 하나의 네트워크 평가만 필요하다.

PASCAL VOC에서 네트워크는 이미지 당 98개의 bounding box를 예측하고 각 상자마다 클래스 확률을 예측한다.

YOLO는 분류기 기반 방법과 달리 하나의 네트워크 평가만 필요하므로 테스트 시에 매우 빠르다.

종종 어떤 grid cell에 객체가 떨어질를 명확하지만,, 네트워크는 객체마다 하나의 상자만 예측한다.

그러나 일부 큰 객체나 여러 셀의 경계에 위치한 객체는 여러 셀이 같은 객체로 인식할 수 있다.

이런 문제는 non-maximal suppression을 통해 개선한다.

YOLO는 비 최대 억제를 통해 mAP를 2~3%가량 향상시켰다.

## 2.4. Limitations of YOLO
YOLO는 각 grid cell이 두 개의 상자를 예측하지만, 하나의 클래스만 가질 수 있다.

이는 bounding box예측에 강력한 공간 제약(spatial constraint)을 가한다.<br>
(공간적 제약이란 '하나의 그리드 셀은 오직 하나의 객체만 검출하므로 하나의 그리드 셀에 두 개 이상의 객체가 붙어있다면 이를 잘 검출하지 못하는 문제'를 뜻합니다. <br>
[출처 : 논문 리뷰 - YOLO(You Only Look Once) 톺아보기](https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once))

무리 지어진 작은 객체, 예를 들어 무리를 이루고 있는 새무리와 같은 경우에 어려움을 겪는다.

또한, 모델은 데이터로부터 bounding box를 예측하는 방식으로 학습하기 때문에 새로운 또는 특이한 종횡비나 배치에 있는 객체에 일반화하기 어렵다.

마지막으로, 우리는 감지 성능을 근사화하는 손실 함수로 훈련하지만, 손실 함수는 작은 bounding box와 큰 bounding box의 오류를 동일하게 처리한다.<br>
큰 상자에서 작은 오차는 일반적으로 미미하지만 작은 상자에서의 작은 오차는 IOU에 더 큰 영향을 미친다.

이를 부정확한 localization 문제라고 부른다.

# 3. Comparison to Other Detection Systems
DPM과 R-CNN과 모델적인 비교를 수행하는데,<br>
간단히 요약하자면

DPM은 슬라이딩 윈도우 사용, DPM은 분리된 파이프라인 사용

하지만 YOLO는 그렇지 않다 합쳐진 파이프라인이며 NMS 같은 것도 한번에 처리한다라고 설명

---

R-CNN과 비교해도,<br>
R-CNN 또한 복잡하고 분리된 파이프라인 사용, 속도 느림<br>
하지만 정확성은 높다.

R-CNN의 bounding box가 2000개 인것에 비해 YOLO는 98개로 훨씬 적다.

---

다른 빠른 탐지기인 Fast R-CNN과 Faster R-CNN은 R-CNN 프레임워크를 가속화하기 위해 연산을 공유하고 선태적 검색(Selective Search) 대신 신경망을 사용하여 영역을 제안하는 방식에 중점을 둔다.<br>

이들은 R-CNN 대비 속도와 정확도 개선을 제공하지만, 여전히 실시간 성능을 달성하지는 못한다.<br>

# 4. Experiments
간단하게 넘어가겠습니다.
<p align="center"><img src="/MyPDF/yolo(6).png" width = "500" ></p>
<p align="center"><img src="/MyPDF/yolo(7).png" width = "500" ></p>
<p align="center"><img src="/MyPDF/yolo(8).png" width = "500" ></p>
<p align="center"><img src="/MyPDF/yolo(9).png" width = "700" ></p>

# 6. Conclusion
모델은 구축하기 간단하며, 전체 이미지에 직접 훈련될 수 있다. 분류기 기반 접근법과 달리 YOLO는 탐지 성능과 직접 관련된 손실 함수로 훈련되며, 전체 모델이 함께 훈련된다.

Fast YOLO는 문헌에서 가장 빠른 범용 객체 탐지기이며, YOLO는 실시간 객체 탐지의 SOTA 수준을 끌어올린다.

YOLO는 새로운 도메인에 대해서도 잘 일반화되므로 빠르고 견고한 객체 탐지에 의존하는 응용 프로그램에 이상적이다.